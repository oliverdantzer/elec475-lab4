{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Model Training on COCO 2014\n",
    "\n",
    "Train a CLIP-style vision-language model with:\n",
    "- **Image Encoder**: ResNet50 (ImageNet pretrained, trainable)\n",
    "- **Projection Head**: 2-layer MLP with GELU (trainable)\n",
    "- **Text Encoder**: CLIP text encoder (frozen)\n",
    "- **Loss**: InfoNCE contrastive loss\n",
    "\n",
    "## Prerequisites\n",
    "Run `coco_dataset_prep.ipynb` first to prepare the dataset and cache text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install -q transformers>=4.30.0 torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q pillow matplotlib tqdm\n",
    "\n",
    "print(\"✓ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'dataset_dir': Path('/content/coco2014'),\n",
    "    'checkpoint_dir': Path('/content/checkpoints'),\n",
    "    'log_dir': Path('/content/logs'),\n",
    "    \n",
    "    # Model\n",
    "    'clip_model_name': 'openai/clip-vit-base-patch32',\n",
    "    'image_size': 224,\n",
    "    'embedding_dim': 512,\n",
    "    'projection_hidden_dim': 1024,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 128,  # Increase if you have more GPU memory\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 500,\n",
    "    'temperature': 0.07,  # Temperature for InfoNCE loss\n",
    "    \n",
    "    # Optimization\n",
    "    'optimizer': 'AdamW',\n",
    "    'scheduler': 'cosine',  # 'cosine' or 'linear'\n",
    "    'max_grad_norm': 1.0,  # Gradient clipping\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    'log_interval': 50,  # Log every N steps\n",
    "    'val_interval': 500,  # Validate every N steps\n",
    "    'save_interval': 1000,  # Save checkpoint every N steps\n",
    "    'num_workers': 2,  # DataLoader workers\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "CONFIG['checkpoint_dir'].mkdir(exist_ok=True, parents=True)\n",
    "CONFIG['log_dir'].mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# CLIP normalization constants\n",
    "CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
    "CLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOClipDataset(Dataset):\n",
    "    \"\"\"COCO Dataset for CLIP fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', dataset_dir=CONFIG['dataset_dir']):\n",
    "        self.split = split\n",
    "        self.image_dir = dataset_dir / f'{split}2014'\n",
    "        self.cache_file = dataset_dir / f'{split}_text_embeddings.pt'\n",
    "        \n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "        ])\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        print(f\"Loading {split} embeddings from {self.cache_file.name}...\")\n",
    "        cache = torch.load(self.cache_file)\n",
    "        self.cache_data = cache['data']\n",
    "        self.embedding_dim = cache['embedding_dim']\n",
    "        \n",
    "        print(f\"  ✓ Loaded {len(self.cache_data):,} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.cache_data[idx]\n",
    "        image_id = item['image_id']\n",
    "        embeddings = item['embeddings']\n",
    "        captions = item['captions']\n",
    "        \n",
    "        # Load image\n",
    "        image_filename = f'COCO_{self.split}2014_{image_id:012d}.jpg'\n",
    "        image_path = self.image_dir / image_filename\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except:\n",
    "            image = torch.zeros(3, CONFIG['image_size'], CONFIG['image_size'])\n",
    "        \n",
    "        # Randomly select one caption\n",
    "        caption_idx = random.randint(0, len(captions) - 1)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text_embedding': embeddings[caption_idx],\n",
    "            'caption': captions[caption_idx],\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50ImageEncoder(nn.Module):\n",
    "    \"\"\"ResNet50 image encoder with ImageNet pretrained weights.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Remove final FC layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.output_dim = 2048\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        return features.view(features.size(0), -1)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"2-layer MLP projection head with GELU activation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"Combined CLIP model for fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, text_encoder, freeze_text_encoder=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (frozen)\n",
    "        self.text_encoder = text_encoder\n",
    "        if freeze_text_encoder:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.text_encoder.eval()\n",
    "        \n",
    "        # Image encoder (trainable)\n",
    "        self.image_encoder = ResNet50ImageEncoder(pretrained=True)\n",
    "        \n",
    "        # Projection head (trainable)\n",
    "        self.projection_head = ProjectionHead(\n",
    "            input_dim=2048,\n",
    "            hidden_dim=CONFIG['projection_hidden_dim'],\n",
    "            output_dim=CONFIG['embedding_dim']\n",
    "        )\n",
    "        \n",
    "    def encode_image(self, images):\n",
    "        \"\"\"Encode images to normalized embeddings.\"\"\"\n",
    "        features = self.image_encoder(images)\n",
    "        embeddings = self.projection_head(features)\n",
    "        # L2 normalize\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        \"\"\"Encode text to normalized embeddings (frozen).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.pooler_output\n",
    "            # L2 normalize\n",
    "            return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass for training.\"\"\"\n",
    "        image_embeddings = self.encode_image(images)\n",
    "        text_embeddings = self.encode_text(input_ids, attention_mask)\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "print(\"✓ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. InfoNCE Loss\n",
    "\n",
    "The InfoNCE (Contrastive) loss used in CLIP:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{2N} \\sum_{i=1}^{N} \\left[ \\log \\frac{\\exp(\\text{sim}(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(I_i, T_j) / \\tau)} + \\log \\frac{\\exp(\\text{sim}(T_i, I_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(T_i, I_j) / \\tau)} \\right]$$\n",
    "\n",
    "where $\\tau$ is the temperature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(nn.Module):\n",
    "    \"\"\"InfoNCE (Contrastive) Loss for CLIP training.\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_embeddings: [batch_size, embedding_dim] - L2 normalized\n",
    "            text_embeddings: [batch_size, embedding_dim] - L2 normalized\n",
    "            \n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        \n",
    "        # Compute similarity matrix: [batch_size, batch_size]\n",
    "        # Since embeddings are L2 normalized, this is cosine similarity\n",
    "        logits = torch.matmul(image_embeddings, text_embeddings.T) / self.temperature\n",
    "        \n",
    "        # Labels: diagonal elements are positive pairs\n",
    "        labels = torch.arange(batch_size, device=logits.device)\n",
    "        \n",
    "        # Symmetric loss: image-to-text + text-to-image\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        \n",
    "        # Compute accuracy for logging\n",
    "        with torch.no_grad():\n",
    "            pred_i2t = torch.argmax(logits, dim=1)\n",
    "            pred_t2i = torch.argmax(logits.T, dim=1)\n",
    "            acc_i2t = (pred_i2t == labels).float().mean()\n",
    "            acc_t2i = (pred_t2i == labels).float().mean()\n",
    "            accuracy = (acc_i2t + acc_t2i) / 2\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "print(\"✓ InfoNCE loss defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and text encoder\n",
    "print(\"Loading CLIP text encoder...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(CONFIG['clip_model_name'])\n",
    "text_encoder = CLIPTextModel.from_pretrained(CONFIG['clip_model_name'])\n",
    "text_encoder = text_encoder.to(device)\n",
    "print(\"✓ Text encoder loaded\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating CLIP model...\")\n",
    "model = CLIPModel(text_encoder=text_encoder, freeze_text_encoder=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "# Loss function\n",
    "criterion = InfoNCELoss(temperature=CONFIG['temperature'])\n",
    "\n",
    "# Optimizer - only optimize trainable parameters\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Optimizer: {CONFIG['optimizer']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Weight decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"  Temperature: {CONFIG['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = COCOClipDataset(split='train')\n",
    "val_dataset = COCOClipDataset(split='val')\n",
    "\n",
    "# Calculate total steps\n",
    "total_steps = len(train_dataset) // CONFIG['batch_size'] * CONFIG['num_epochs']\n",
    "\n",
    "# Cosine annealing with warmup\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Create a schedule with linear warmup and cosine annealing.\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Learning rate scheduler:\")\n",
    "print(f\"  Type: Cosine with warmup\")\n",
    "print(f\"  Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "print(f\"  Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, epoch, global_step, history):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    # Keep text encoder in eval mode\n",
    "    model.text_encoder.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy = criterion(image_embeddings, text_embeddings)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{accuracy.item():.3f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # Log\n",
    "        if global_step % CONFIG['log_interval'] == 0:\n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['train_acc'].append(accuracy.item())\n",
    "            history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "            history['step'].append(global_step)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_acc = epoch_acc / len(train_loader)\n",
    "    \n",
    "    return avg_loss, avg_acc, global_step\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy = criterion(image_embeddings, text_embeddings)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_acc = total_acc / len(val_loader)\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rate': [],\n",
    "    'step': [],\n",
    "    'epoch': []\n",
    "}\n",
    "\n",
    "# Hardware info\n",
    "hardware_info = {\n",
    "    'device': str(device),\n",
    "    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "    'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting Training\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Hardware: {hardware_info['gpu_name']}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "try:\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        # Train\n",
    "        train_loss, train_acc, global_step = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, epoch, global_step, history\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "        \n",
    "        # Log epoch results\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['epoch'].append(epoch)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'config': CONFIG,\n",
    "            }, CONFIG['checkpoint_dir'] / 'best_model.pt')\n",
    "            print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'history': history,\n",
    "        }, CONFIG['checkpoint_dir'] / f'checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user\")\n",
    "\n",
    "finally:\n",
    "    # Training complete\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Final learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save training history and hardware info\n",
    "    training_summary = {\n",
    "        'history': history,\n",
    "        'hardware_info': hardware_info,\n",
    "        'training_time_hours': total_time / 3600,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': CONFIG,\n",
    "    }\n",
    "    \n",
    "    torch.save(training_summary, CONFIG['log_dir'] / 'training_summary.pt')\n",
    "    print(f\"\\n✓ Training summary saved to {CONFIG['log_dir'] / 'training_summary.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(history['step'], history['train_loss'], alpha=0.6, label='Train Loss (per step)')\n",
    "axes[0, 0].set_xlabel('Steps')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy\n",
    "axes[0, 1].plot(history['step'], history['train_acc'], alpha=0.6, label='Train Accuracy')\n",
    "axes[0, 1].set_xlabel('Steps')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "axes[1, 0].plot(history['epoch'], history['val_loss'], 'o-', label='Val Loss (per epoch)', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Validation Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[1, 1].plot(history['epoch'], history['val_acc'], 'o-', label='Val Accuracy (per epoch)', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_title('Validation Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['log_dir'] / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {CONFIG['log_dir'] / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING REPORT\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Hardware Information:\")\n",
    "print(f\"  Device: {hardware_info['device']}\")\n",
    "print(f\"  GPU: {hardware_info['gpu_name']}\")\n",
    "print(f\"  GPU Memory: {hardware_info['gpu_memory_gb']:.2f} GB\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Weight decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
    "print(f\"  Optimizer: {CONFIG['optimizer']}\")\n",
    "print(f\"  Scheduler: {CONFIG['scheduler']} with {CONFIG['warmup_steps']} warmup steps\")\n",
    "print(f\"  Gradient clipping: {CONFIG['max_grad_norm']}\")\n",
    "\n",
    "print(\"\\nTraining Results:\")\n",
    "print(f\"  Total training time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Final validation accuracy: {history['val_acc'][-1]:.3f}\")\n",
    "print(f\"  Total steps: {global_step:,}\")\n",
    "\n",
    "print(\"\\nObserved Issues:\")\n",
    "# Check for common issues\n",
    "if max(history['train_loss']) > 10:\n",
    "    print(\"  ⚠️ High initial loss detected - may need learning rate adjustment\")\n",
    "if history['val_loss'][-1] > history['val_loss'][0]:\n",
    "    print(\"  ⚠️ Validation loss increased - possible overfitting\")\n",
    "if best_val_loss == history['val_loss'][-1]:\n",
    "    print(\"  ✓ Best model is from the final epoch - training is converging well\")\n",
    "else:\n",
    "    best_epoch = history['epoch'][history['val_loss'].index(best_val_loss)]\n",
    "    print(f\"  ℹ️ Best model from epoch {best_epoch+1}, consider early stopping\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Save report to file\n",
    "with open(CONFIG['log_dir'] / 'training_report.txt', 'w') as f:\n",
    "    f.write(f\"CLIP Training Report\\n\")\n",
    "    f.write(f\"{'='*60}\\n\\n\")\n",
    "    f.write(f\"Hardware: {hardware_info['gpu_name']}\\n\")\n",
    "    f.write(f\"Training time: {total_time/3600:.2f} hours\\n\")\n",
    "    f.write(f\"Best val loss: {best_val_loss:.4f}\\n\")\n",
    "    f.write(f\"Final val accuracy: {history['val_acc'][-1]:.3f}\\n\")\n",
    "\n",
    "print(f\"\\n✓ Training report saved to {CONFIG['log_dir'] / 'training_report.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

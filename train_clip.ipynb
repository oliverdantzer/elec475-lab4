{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Model Training on COCO 2014\n",
    "\n",
    "Train a CLIP-style vision-language model with:\n",
    "- **Image Encoder**: ResNet50 (ImageNet pretrained, trainable)\n",
    "- **Projection Head**: 2-layer MLP with GELU (trainable)\n",
    "- **Text Encoder**: CLIP text encoder (frozen)\n",
    "- **Loss**: InfoNCE contrastive loss\n",
    "\n",
    "## Prerequisites\n",
    "Run `coco_dataset_prep.ipynb` first to prepare the dataset and cache text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install -q transformers>=4.30.0 torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q pillow matplotlib tqdm\n",
    "\n",
    "print(\"\u2713 All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nCONFIG = {\n    # Paths\n    'dataset_dir': Path('/content/coco2014'),\n    'checkpoint_dir': Path('/content/checkpoints'),\n    'log_dir': Path('/content/logs'),\n    \n    # Model\n    'clip_model_name': 'openai/clip-vit-base-patch32',\n    'image_size': 224,\n    'embedding_dim': 512,\n    'projection_hidden_dim': 1024,\n    \n    # === EXPERIMENTAL FLAGS ===\n    # Set these to True to enable modifications\n    'use_batch_norm': False,        # Add BatchNorm to projection head for better training stability\n    'use_attention_pooling': False, # Use attention pooling instead of avg pooling in ResNet50\n    # ==========================\n    \n    # Training hyperparameters\n    'batch_size': 128,  # Increase if you have more GPU memory\n    'num_epochs': 10,\n    'learning_rate': 1e-4,\n    'weight_decay': 0.01,\n    'warmup_steps': 500,\n    'temperature': 0.07,  # Temperature for InfoNCE loss\n    \n    # Optimization\n    'optimizer': 'AdamW',\n    'scheduler': 'cosine',  # 'cosine' or 'linear'\n    'max_grad_norm': 1.0,  # Gradient clipping\n    \n    # Logging and checkpointing\n    'log_interval': 50,  # Log every N steps\n    'val_interval': 500,  # Validate every N steps\n    'save_interval': 1000,  # Save checkpoint every N steps\n    'num_workers': 2,  # DataLoader workers\n}\n\n# Create directories with flag suffixes\nflags_suffix = \"\"\nif CONFIG['use_batch_norm']:\n    flags_suffix += \"_bn\"\nif CONFIG['use_attention_pooling']:\n    flags_suffix += \"_attn\"\n\n# Update directory names to include flags\nCONFIG['checkpoint_dir'] = Path(f\"/content/checkpoints{flags_suffix}\")\nCONFIG['log_dir'] = Path(f\"/content/logs{flags_suffix}\")\n\nCONFIG['checkpoint_dir'].mkdir(exist_ok=True, parents=True)\nCONFIG['log_dir'].mkdir(exist_ok=True, parents=True)\n\n# CLIP normalization constants\nCLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\nCLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n\nprint(\"Configuration:\")\nfor key, value in CONFIG.items():\n    if key in ['use_batch_norm', 'use_attention_pooling']:\n        marker = \"\u2713\" if value else \"\u2717\"\n        print(f\"  {marker} {key}: {value}\")\n    else:\n        print(f\"  {key}: {value}\")\n\nif CONFIG['use_batch_norm'] or CONFIG['use_attention_pooling']:\n    print(f\"\\n\ud83d\udd2c Experimental modifications enabled!\")\n    print(f\"   Outputs will be saved to: *{flags_suffix}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOClipDataset(Dataset):\n",
    "    \"\"\"COCO Dataset for CLIP fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', dataset_dir=CONFIG['dataset_dir']):\n",
    "        self.split = split\n",
    "        self.image_dir = dataset_dir / f'{split}2014'\n",
    "        self.cache_file = dataset_dir / f'{split}_text_embeddings.pt'\n",
    "        \n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "        ])\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        print(f\"Loading {split} embeddings from {self.cache_file.name}...\")\n",
    "        cache = torch.load(self.cache_file)\n",
    "        self.cache_data = cache['data']\n",
    "        self.embedding_dim = cache['embedding_dim']\n",
    "        \n",
    "        print(f\"  \u2713 Loaded {len(self.cache_data):,} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.cache_data[idx]\n",
    "        image_id = item['image_id']\n",
    "        embeddings = item['embeddings']\n",
    "        captions = item['captions']\n",
    "        \n",
    "        # Load image\n",
    "        image_filename = f'COCO_{self.split}2014_{image_id:012d}.jpg'\n",
    "        image_path = self.image_dir / image_filename\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except:\n",
    "            image = torch.zeros(3, CONFIG['image_size'], CONFIG['image_size'])\n",
    "        \n",
    "        # Randomly select one caption\n",
    "        caption_idx = random.randint(0, len(captions) - 1)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text_embedding': embeddings[caption_idx],\n",
    "            'caption': captions[caption_idx],\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "print(\"\u2713 Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AttentionPooling(nn.Module):\n    \"\"\"Attention-based pooling for better feature aggregation.\"\"\"\n    def __init__(self, input_dim=2048):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, input_dim // 8),\n            nn.Tanh(),\n            nn.Linear(input_dim // 8, 1)\n        )\n    \n    def forward(self, x):\n        # x: [B, C, H, W]\n        B, C, H, W = x.shape\n        x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # [B, H*W, C]\n        \n        # Compute attention weights\n        attn_weights = self.attention(x_flat)  # [B, H*W, 1]\n        attn_weights = F.softmax(attn_weights, dim=1)\n        \n        # Weighted sum\n        pooled = torch.sum(x_flat * attn_weights, dim=1)  # [B, C]\n        return pooled\n\n\nclass ResNet50ImageEncoder(nn.Module):\n    \"\"\"ResNet50 image encoder with optional attention pooling.\"\"\"\n    \n    def __init__(self, pretrained=True, use_attention_pooling=False):\n        super().__init__()\n        resnet = models.resnet50(pretrained=pretrained)\n        \n        if use_attention_pooling:\n            # Remove avgpool and FC layer, use attention pooling instead\n            self.features = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n            self.pooling = AttentionPooling(input_dim=2048)\n            self.use_attention = True\n            print(\"  Using attention pooling (experimental)\")\n        else:\n            # Standard: remove only FC layer, keep avgpool\n            self.features = nn.Sequential(*list(resnet.children())[:-1])\n            self.pooling = None\n            self.use_attention = False\n        \n        self.output_dim = 2048\n        \n    def forward(self, x):\n        features = self.features(x)\n        \n        if self.use_attention:\n            # features: [B, 2048, 7, 7]\n            pooled = self.pooling(features)  # [B, 2048]\n        else:\n            # features: [B, 2048, 1, 1]\n            pooled = features.view(features.size(0), -1)\n        \n        return pooled\n\n\nclass ProjectionHead(nn.Module):\n    \"\"\"2-layer MLP projection head with optional BatchNorm.\"\"\"\n    \n    def __init__(self, input_dim=2048, hidden_dim=1024, output_dim=512, use_batch_norm=False):\n        super().__init__()\n        \n        if use_batch_norm:\n            # With BatchNorm for better training stability\n            self.projection = nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.GELU(),\n                nn.Linear(hidden_dim, output_dim),\n                nn.BatchNorm1d(output_dim)\n            )\n            print(\"  Using BatchNorm in projection head (experimental)\")\n        else:\n            # Standard: no BatchNorm\n            self.projection = nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.GELU(),\n                nn.Linear(hidden_dim, output_dim)\n            )\n        \n    def forward(self, x):\n        return self.projection(x)\n\n\nclass CLIPModel(nn.Module):\n    \"\"\"Combined CLIP model with experimental modifications.\"\"\"\n    \n    def __init__(self, text_encoder, freeze_text_encoder=True, \n                 use_batch_norm=False, use_attention_pooling=False):\n        super().__init__()\n        \n        # Store configuration\n        self.use_batch_norm = use_batch_norm\n        self.use_attention_pooling = use_attention_pooling\n        \n        # Text encoder (frozen)\n        self.text_encoder = text_encoder\n        if freeze_text_encoder:\n            for param in self.text_encoder.parameters():\n                param.requires_grad = False\n            self.text_encoder.eval()\n        \n        # Image encoder (trainable) with optional attention pooling\n        self.image_encoder = ResNet50ImageEncoder(\n            pretrained=True,\n            use_attention_pooling=use_attention_pooling\n        )\n        \n        # Projection head (trainable) with optional BatchNorm\n        self.projection_head = ProjectionHead(\n            input_dim=2048,\n            hidden_dim=CONFIG['projection_hidden_dim'],\n            output_dim=CONFIG['embedding_dim'],\n            use_batch_norm=use_batch_norm\n        )\n        \n    def encode_image(self, images):\n        \"\"\"Encode images to normalized embeddings.\"\"\"\n        features = self.image_encoder(images)\n        embeddings = self.projection_head(features)\n        # L2 normalize\n        return F.normalize(embeddings, p=2, dim=1)\n    \n    def encode_text(self, input_ids, attention_mask):\n        \"\"\"Encode text to normalized embeddings (frozen).\"\"\"\n        with torch.no_grad():\n            outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n            embeddings = outputs.pooler_output\n            # L2 normalize\n            return F.normalize(embeddings, p=2, dim=1)\n    \n    def forward(self, images, input_ids, attention_mask):\n        \"\"\"Forward pass for training.\"\"\"\n        image_embeddings = self.encode_image(images)\n        text_embeddings = self.encode_text(input_ids, attention_mask)\n        return image_embeddings, text_embeddings\n\nprint(\"\u2713 Model architecture defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. InfoNCE Loss\n",
    "\n",
    "The InfoNCE (Contrastive) loss used in CLIP:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{2N} \\sum_{i=1}^{N} \\left[ \\log \\frac{\\exp(\\text{sim}(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(I_i, T_j) / \\tau)} + \\log \\frac{\\exp(\\text{sim}(T_i, I_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(T_i, I_j) / \\tau)} \\right]$$\n",
    "\n",
    "where $\\tau$ is the temperature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(nn.Module):\n",
    "    \"\"\"InfoNCE (Contrastive) Loss for CLIP training.\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_embeddings: [batch_size, embedding_dim] - L2 normalized\n",
    "            text_embeddings: [batch_size, embedding_dim] - L2 normalized\n",
    "            \n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        \n",
    "        # Compute similarity matrix: [batch_size, batch_size]\n",
    "        # Since embeddings are L2 normalized, this is cosine similarity\n",
    "        logits = torch.matmul(image_embeddings, text_embeddings.T) / self.temperature\n",
    "        \n",
    "        # Labels: diagonal elements are positive pairs\n",
    "        labels = torch.arange(batch_size, device=logits.device)\n",
    "        \n",
    "        # Symmetric loss: image-to-text + text-to-image\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        \n",
    "        # Compute accuracy for logging\n",
    "        with torch.no_grad():\n",
    "            pred_i2t = torch.argmax(logits, dim=1)\n",
    "            pred_t2i = torch.argmax(logits.T, dim=1)\n",
    "            acc_i2t = (pred_i2t == labels).float().mean()\n",
    "            acc_t2i = (pred_t2i == labels).float().mean()\n",
    "            accuracy = (acc_i2t + acc_t2i) / 2\n",
    "        \n",
    "        return loss, accuracy\n",
    "\n",
    "print(\"\u2713 InfoNCE loss defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer and text encoder\nprint(\"Loading CLIP text encoder...\")\ntokenizer = CLIPTokenizer.from_pretrained(CONFIG['clip_model_name'])\ntext_encoder = CLIPTextModel.from_pretrained(CONFIG['clip_model_name'])\ntext_encoder = text_encoder.to(device)\nprint(\"\u2713 Text encoder loaded\")\n\n# Create model with experimental flags\nprint(\"\\nCreating CLIP model...\")\nmodel = CLIPModel(\n    text_encoder=text_encoder, \n    freeze_text_encoder=True,\n    use_batch_norm=CONFIG['use_batch_norm'],\n    use_attention_pooling=CONFIG['use_attention_pooling']\n)\nmodel = model.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\nModel Statistics:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n\n# Print modifications\nif CONFIG['use_batch_norm'] or CONFIG['use_attention_pooling']:\n    print(f\"\\n\ud83d\udd2c Experimental Modifications:\")\n    if CONFIG['use_batch_norm']:\n        print(f\"  \u2713 BatchNorm in projection head\")\n    if CONFIG['use_attention_pooling']:\n        print(f\"  \u2713 Attention pooling in image encoder\")\n\n# Loss function\ncriterion = InfoNCELoss(temperature=CONFIG['temperature'])\n\n# Optimizer - only optimize trainable parameters\noptimizer = torch.optim.AdamW(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=CONFIG['learning_rate'],\n    weight_decay=CONFIG['weight_decay']\n)\n\nprint(f\"\\n\u2713 Optimizer: {CONFIG['optimizer']}\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']}\")\nprint(f\"  Weight decay: {CONFIG['weight_decay']}\")\nprint(f\"  Temperature: {CONFIG['temperature']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = COCOClipDataset(split='train')\n",
    "val_dataset = COCOClipDataset(split='val')\n",
    "\n",
    "# Calculate total steps\n",
    "total_steps = len(train_dataset) // CONFIG['batch_size'] * CONFIG['num_epochs']\n",
    "\n",
    "# Cosine annealing with warmup\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Create a schedule with linear warmup and cosine annealing.\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=CONFIG['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Learning rate scheduler:\")\n",
    "print(f\"  Type: Cosine with warmup\")\n",
    "print(f\"  Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "print(f\"  Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, epoch, global_step, history):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    # Keep text encoder in eval mode\n",
    "    model.text_encoder.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy = criterion(image_embeddings, text_embeddings)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{accuracy.item():.3f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "        \n",
    "        # Log\n",
    "        if global_step % CONFIG['log_interval'] == 0:\n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['train_acc'].append(accuracy.item())\n",
    "            history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "            history['step'].append(global_step)\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_acc = epoch_acc / len(train_loader)\n",
    "    \n",
    "    return avg_loss, avg_acc, global_step\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, criterion):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, accuracy = criterion(image_embeddings, text_embeddings)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_acc = total_acc / len(val_loader)\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(\"\u2713 Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    num_workers=CONFIG['num_workers'],\n    pin_memory=True,\n    drop_last=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=CONFIG['num_workers'],\n    pin_memory=True\n)\n\nprint(f\"Data loaders created:\")\nprint(f\"  Train batches: {len(train_loader):,}\")\nprint(f\"  Val batches: {len(val_loader):,}\")\nprint(f\"  Batch size: {CONFIG['batch_size']}\")\n\n# Create flags suffix for filenames\nflags_suffix = \"\"\nif CONFIG['use_batch_norm']:\n    flags_suffix += \"_bn\"\nif CONFIG['use_attention_pooling']:\n    flags_suffix += \"_attn\"\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': [],\n    'learning_rate': [],\n    'step': [],\n    'epoch': [],\n    'flags': {\n        'use_batch_norm': CONFIG['use_batch_norm'],\n        'use_attention_pooling': CONFIG['use_attention_pooling']\n    }\n}\n\n# Hardware info\nhardware_info = {\n    'device': str(device),\n    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n    'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\n}\n\nprint(f\"\\n{'='*60}\")\nprint(\"Starting Training\")\nprint(f\"{'='*60}\")\nprint(f\"Hardware: {hardware_info['gpu_name']}\")\nprint(f\"Epochs: {CONFIG['num_epochs']}\")\nprint(f\"Batch size: {CONFIG['batch_size']}\")\nprint(f\"Learning rate: {CONFIG['learning_rate']}\")\nif flags_suffix:\n    print(f\"Experimental flags: {flags_suffix}\")\nprint(f\"{'='*60}\\n\")\n\n# Start training\nstart_time = time.time()\nglobal_step = 0\nbest_val_loss = float('inf')\n\ntry:\n    for epoch in range(CONFIG['num_epochs']):\n        # Train\n        train_loss, train_acc, global_step = train_epoch(\n            model, train_loader, criterion, optimizer, scheduler, epoch, global_step, history\n        )\n        \n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion)\n        \n        # Log epoch results\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['epoch'].append(epoch)\n        \n        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}:\")\n        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\")\n        \n        # Save best model with flags suffix\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_loss': val_loss,\n                'val_acc': val_acc,\n                'config': CONFIG,\n                'flags': {\n                    'use_batch_norm': CONFIG['use_batch_norm'],\n                    'use_attention_pooling': CONFIG['use_attention_pooling']\n                }\n            }\n            torch.save(checkpoint, CONFIG['checkpoint_dir'] / f'best_model{flags_suffix}.pt')\n            print(f\"  \u2713 Saved best model (val_loss: {val_loss:.4f}) \u2192 best_model{flags_suffix}.pt\")\n        \n        # Save checkpoint\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'history': history,\n            'config': CONFIG,\n        }, CONFIG['checkpoint_dir'] / f'checkpoint_epoch_{epoch+1}{flags_suffix}.pt')\n\nexcept KeyboardInterrupt:\n    print(\"\\n\\nTraining interrupted by user\")\n\nfinally:\n    # Training complete\n    total_time = time.time() - start_time\n    \n    print(f\"\\n{'='*60}\")\n    print(\"Training Complete!\")\n    print(f\"{'='*60}\")\n    print(f\"Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n    print(f\"Final learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n    if flags_suffix:\n        print(f\"Experimental flags: {flags_suffix}\")\n    print(f\"{'='*60}\")\n    \n    # Save training history and hardware info\n    training_summary = {\n        'history': history,\n        'hardware_info': hardware_info,\n        'training_time_hours': total_time / 3600,\n        'best_val_loss': best_val_loss,\n        'config': CONFIG,\n        'flags': {\n            'use_batch_norm': CONFIG['use_batch_norm'],\n            'use_attention_pooling': CONFIG['use_attention_pooling'],\n            'suffix': flags_suffix\n        }\n    }\n    \n    torch.save(training_summary, CONFIG['log_dir'] / f'training_summary{flags_suffix}.pt')\n    print(f\"\\n\u2713 Training summary saved to {CONFIG['log_dir'] / f'training_summary{flags_suffix}.pt'}\") "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create flags suffix for plot filename\nflags_suffix = \"\"\nif CONFIG['use_batch_norm']:\n    flags_suffix += \"_bn\"\nif CONFIG['use_attention_pooling']:\n    flags_suffix += \"_attn\"\n\n# Plot loss curves\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Add main title with flags\nmain_title = \"Training Curves\"\nif flags_suffix:\n    main_title += f\" (Modifications: {flags_suffix.replace('_', ' ').strip()})\"\nfig.suptitle(main_title, fontsize=16, fontweight='bold')\n\n# Training loss\naxes[0, 0].plot(history['step'], history['train_loss'], alpha=0.6, label='Train Loss (per step)')\naxes[0, 0].set_xlabel('Steps')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Training accuracy\naxes[0, 1].plot(history['step'], history['train_acc'], alpha=0.6, label='Train Accuracy')\naxes[0, 1].set_xlabel('Steps')\naxes[0, 1].set_ylabel('Accuracy')\naxes[0, 1].set_title('Training Accuracy')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Validation loss\naxes[1, 0].plot(history['epoch'], history['val_loss'], 'o-', label='Val Loss (per epoch)', linewidth=2)\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].set_title('Validation Loss')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Validation accuracy\naxes[1, 1].plot(history['epoch'], history['val_acc'], 'o-', label='Val Accuracy (per epoch)', linewidth=2)\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Accuracy')\naxes[1, 1].set_title('Validation Accuracy')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplot_filename = f'training_curves{flags_suffix}.png'\nplt.savefig(CONFIG['log_dir'] / plot_filename, dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\u2713 Training curves saved to {CONFIG['log_dir'] / plot_filename}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create flags suffix\nflags_suffix = \"\"\nif CONFIG['use_batch_norm']:\n    flags_suffix += \"_bn\"\nif CONFIG['use_attention_pooling']:\n    flags_suffix += \"_attn\"\n\n# Generate training report\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING REPORT\")\nprint(f\"{'='*60}\\n\")\n\nprint(\"Hardware Information:\")\nprint(f\"  Device: {hardware_info['device']}\")\nprint(f\"  GPU: {hardware_info['gpu_name']}\")\nprint(f\"  GPU Memory: {hardware_info['gpu_memory_gb']:.2f} GB\")\n\nprint(\"\\nTraining Configuration:\")\nprint(f\"  Epochs: {CONFIG['num_epochs']}\")\nprint(f\"  Batch size: {CONFIG['batch_size']}\")\nprint(f\"  Learning rate: {CONFIG['learning_rate']}\")\nprint(f\"  Weight decay: {CONFIG['weight_decay']}\")\nprint(f\"  Temperature: {CONFIG['temperature']}\")\nprint(f\"  Optimizer: {CONFIG['optimizer']}\")\nprint(f\"  Scheduler: {CONFIG['scheduler']} with {CONFIG['warmup_steps']} warmup steps\")\nprint(f\"  Gradient clipping: {CONFIG['max_grad_norm']}\")\n\nprint(\"\\n\ud83d\udd2c Experimental Modifications:\")\nif CONFIG['use_batch_norm']:\n    print(f\"  \u2713 BatchNorm in projection head\")\nelse:\n    print(f\"  \u2717 BatchNorm in projection head\")\nif CONFIG['use_attention_pooling']:\n    print(f\"  \u2713 Attention pooling in image encoder\")\nelse:\n    print(f\"  \u2717 Attention pooling in image encoder\")\nif flags_suffix:\n    print(f\"  Suffix: {flags_suffix}\")\n\nprint(\"\\nTraining Results:\")\nprint(f\"  Total training time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\nprint(f\"  Best validation loss: {best_val_loss:.4f}\")\nprint(f\"  Final validation loss: {history['val_loss'][-1]:.4f}\")\nprint(f\"  Final validation accuracy: {history['val_acc'][-1]:.3f}\")\nprint(f\"  Total steps: {global_step:,}\")\n\nprint(\"\\nObserved Issues:\")\n# Check for common issues\nif max(history['train_loss']) > 10:\n    print(\"  \u26a0\ufe0f High initial loss detected - may need learning rate adjustment\")\nif history['val_loss'][-1] > history['val_loss'][0]:\n    print(\"  \u26a0\ufe0f Validation loss increased - possible overfitting\")\nif best_val_loss == history['val_loss'][-1]:\n    print(\"  \u2713 Best model is from the final epoch - training is converging well\")\nelse:\n    best_epoch = history['epoch'][history['val_loss'].index(best_val_loss)]\n    print(f\"  \u2139\ufe0f Best model from epoch {best_epoch+1}, consider early stopping\")\n\nprint(f\"\\n{'='*60}\")\n\n# Save report to file with flags suffix\nreport_filename = f'training_report{flags_suffix}.txt'\nwith open(CONFIG['log_dir'] / report_filename, 'w') as f:\n    f.write(f\"CLIP Training Report\\n\")\n    f.write(f\"{'='*60}\\n\\n\")\n    f.write(f\"Hardware: {hardware_info['gpu_name']}\\n\")\n    f.write(f\"Training time: {total_time/3600:.2f} hours\\n\")\n    f.write(f\"Best val loss: {best_val_loss:.4f}\\n\")\n    f.write(f\"Final val accuracy: {history['val_acc'][-1]:.3f}\\n\")\n    f.write(f\"\\nExperimental Modifications:\\n\")\n    f.write(f\"  BatchNorm: {CONFIG['use_batch_norm']}\\n\")\n    f.write(f\"  Attention Pooling: {CONFIG['use_attention_pooling']}\\n\")\n    if flags_suffix:\n        f.write(f\"  Suffix: {flags_suffix}\\n\")\n\nprint(f\"\\n\u2713 Training report saved to {CONFIG['log_dir'] / report_filename}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Fine-tuning on MS COCO 2014\n",
    "\n",
    "This notebook provides a complete pipeline for:\n",
    "1. Dataset preparation\n",
    "2. Model training\n",
    "3. Evaluation and visualization\n",
    "\n",
    "All complex code is in Python modules:\n",
    "- `config.py` - Configuration and constants\n",
    "- `models.py` - Model architectures\n",
    "- `dataset.py` - Dataset classes\n",
    "- `utils.py` - Training, evaluation, and visualization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
    "import colab_github\n",
    "colab_github.github_auth(persistent_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:oliverdantzer/elec475-lab4.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/elec475-lab4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.system(\"pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Import from our modules\n",
    "from config import *\n",
    "from models import CLIPModel, InfoNCELoss\n",
    "from dataset import COCOClipDataset, get_clip_transforms\n",
    "from utils import (\n",
    "    encode_and_cache_captions,\n",
    "    setup_optimizer_and_scheduler,\n",
    "    train_epoch,\n",
    "    validate,\n",
    "    compute_embeddings,\n",
    "    compute_recall_at_k,\n",
    "    retrieve_images_for_text,\n",
    "    zero_shot_classify,\n",
    "    plot_training_curves,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    visualize_samples\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set all hyperparameters and experimental flags here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = DEFAULT_CONFIG.copy()\n",
    "\n",
    "# Paths (customize these for your environment)\n",
    "CONFIG['dataset_dir'] = Path('/content/coco2014')\n",
    "CONFIG['checkpoint_dir'] = Path('/content/checkpoints')\n",
    "CONFIG['log_dir'] = Path('/content/logs')\n",
    "\n",
    "# Training hyperparameters\n",
    "CONFIG['batch_size'] = 128\n",
    "CONFIG['num_epochs'] = 10\n",
    "CONFIG['learning_rate'] = 1e-4\n",
    "CONFIG['weight_decay'] = 0.01\n",
    "CONFIG['warmup_steps'] = 500\n",
    "CONFIG['temperature'] = 0.07\n",
    "CONFIG['max_grad_norm'] = 1.0\n",
    "\n",
    "# ===== EXPERIMENTAL FLAGS =====\n",
    "# Set these to True to enable architectural modifications\n",
    "CONFIG['use_batch_norm'] = False        # Add BatchNorm to projection head\n",
    "CONFIG['use_attention_pooling'] = False  # Use attention pooling in ResNet50\n",
    "# ==============================\n",
    "\n",
    "# Create directories with flags suffix\n",
    "flags_suffix = \"\"\n",
    "if CONFIG['use_batch_norm']:\n",
    "    flags_suffix += \"_bn\"\n",
    "if CONFIG['use_attention_pooling']:\n",
    "    flags_suffix += \"_attn\"\n",
    "\n",
    "if flags_suffix:\n",
    "    CONFIG['checkpoint_dir'] = Path(str(CONFIG['checkpoint_dir']) + flags_suffix)\n",
    "    CONFIG['log_dir'] = Path(str(CONFIG['log_dir']) + flags_suffix)\n",
    "\n",
    "CONFIG['checkpoint_dir'].mkdir(exist_ok=True, parents=True)\n",
    "CONFIG['log_dir'].mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Print configuration\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {CONFIG['dataset_dir']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Temperature: {CONFIG['temperature']}\")\n",
    "print(f\"\\n  Experimental Flags:\")\n",
    "print(f\"    BatchNorm: {CONFIG['use_batch_norm']}\")\n",
    "print(f\"    Attention Pooling: {CONFIG['use_attention_pooling']}\")\n",
    "if flags_suffix:\n",
    "    print(f\"    Suffix: {flags_suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation (Optional)\n",
    "\n",
    "Run this section if you haven't prepared the dataset yet. Skip if text embeddings are already cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download COCO 2014 dataset using kagglehub\nimport kagglehub\nimport shutil\n\nannotations_dir = CONFIG['dataset_dir'] / 'annotations'\ntrain_images_dir = CONFIG['dataset_dir'] / 'train2014'\nval_images_dir = CONFIG['dataset_dir'] / 'val2014'\n\nif not (annotations_dir.exists() and train_images_dir.exists() and val_images_dir.exists()):\n    print(\"Downloading COCO 2014 dataset from Kaggle...\")\n    \n    # Download dataset\n    path = kagglehub.dataset_download(\"jeffaudi/coco-2014-dataset-for-yolov3\")\n    print(f\"Downloaded to: {path}\")\n    \n    # Show what's in the downloaded directory\n    downloaded_path = Path(path)\n    print(f\"\\nDownloaded dataset structure:\")\n    for item in downloaded_path.iterdir():\n        print(f\"  - {item.name}\")\n    \n    # Create our dataset directory\n    CONFIG['dataset_dir'].mkdir(exist_ok=True, parents=True)\n    \n    # Copy or symlink annotations\n    if not annotations_dir.exists():\n        source_annotations = downloaded_path / 'annotations'\n        if source_annotations.exists():\n            print(f\"\\nCopying annotations to {annotations_dir}...\")\n            shutil.copytree(source_annotations, annotations_dir)\n            print(\"✓ Annotations copied\")\n        else:\n            print(f\"⚠️ Warning: Annotations not found at {source_annotations}\")\n    \n    # Copy or symlink train images\n    if not train_images_dir.exists():\n        source_train = downloaded_path / 'train2014'\n        if source_train.exists():\n            print(f\"\\nCreating symlink for train images...\")\n            train_images_dir.symlink_to(source_train)\n            print(f\"✓ Train images linked: {train_images_dir} -> {source_train}\")\n        else:\n            print(f\"⚠️ Warning: train2014 not found. Available: {list(downloaded_path.iterdir())}\")\n    \n    # Copy or symlink val images\n    if not val_images_dir.exists():\n        source_val = downloaded_path / 'val2014'\n        if source_val.exists():\n            print(f\"\\nCreating symlink for val images...\")\n            val_images_dir.symlink_to(source_val)\n            print(f\"✓ Val images linked: {val_images_dir} -> {source_val}\")\n        else:\n            print(f\"⚠️ Warning: val2014 not found. Available: {list(downloaded_path.iterdir())}\")\nelse:\n    print(\"✓ Dataset directories already exist\")\n    print(f\"  Annotations: {annotations_dir}\")\n    print(f\"  Train images: {train_images_dir}\")\n    print(f\"  Val images: {val_images_dir}\")\n\n# Check if dataset needs preparation\ntrain_cache = CONFIG['dataset_dir'] / 'train_text_embeddings.pt'\nval_cache = CONFIG['dataset_dir'] / 'val_text_embeddings.pt'\n\nif train_cache.exists() and val_cache.exists():\n    print(\"\\n✓ Dataset already prepared!\")\n    print(f\"  Train cache: {train_cache}\")\n    print(f\"  Val cache: {val_cache}\")\nelse:\n    print(\"\\nDataset needs preparation. Loading CLIP text encoder...\")\n    \n    # Load text encoder for encoding captions\n    tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n    text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\n    text_encoder = text_encoder.to(device)\n    \n    # Encode and cache training captions\n    if not train_cache.exists():\n        print(\"\\nEncoding training captions...\")\n        encode_and_cache_captions(\n            split='train',\n            dataset_dir=CONFIG['dataset_dir'],\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            device=device\n        )\n    \n    # Encode and cache validation captions\n    if not val_cache.exists():\n        print(\"\\nEncoding validation captions...\")\n        encode_and_cache_captions(\n            split='val',\n            dataset_dir=CONFIG['dataset_dir'],\n            tokenizer=tokenizer,\n            text_encoder=text_encoder,\n            device=device\n        )\n    \n    print(\"\\n✓ Dataset preparation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = COCOClipDataset(split='train', dataset_dir=CONFIG['dataset_dir'])\n",
    "val_dataset = COCOClipDataset(split='val', dataset_dir=CONFIG['dataset_dir'])\n",
    "\n",
    "print(f\"\\n✓ Datasets loaded:\")\n",
    "print(f\"  Train: {len(train_dataset):,} images\")\n",
    "print(f\"  Val: {len(val_dataset):,} images\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random samples\n",
    "visualize_samples(train_dataset, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP text encoder\n",
    "print(\"Loading CLIP text encoder...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "# Create CLIP model with experimental flags\n",
    "print(\"\\nCreating CLIP model...\")\n",
    "model = CLIPModel(\n",
    "    text_encoder=text_encoder,\n",
    "    freeze_text_encoder=True,\n",
    "    use_batch_norm=CONFIG['use_batch_norm'],\n",
    "    use_attention_pooling=CONFIG['use_attention_pooling']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ Model created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "\n",
    "if CONFIG['use_batch_norm'] or CONFIG['use_attention_pooling']:\n",
    "    print(f\"\\n  Experimental modifications:\")\n",
    "    if CONFIG['use_batch_norm']:\n",
    "        print(f\"    ✓ BatchNorm in projection head\")\n",
    "    if CONFIG['use_attention_pooling']:\n",
    "        print(f\"    ✓ Attention pooling in image encoder\")\n",
    "\n",
    "# Create loss function\n",
    "criterion = InfoNCELoss(temperature=CONFIG['temperature'])\n",
    "print(f\"\\n✓ Loss function: InfoNCE (temperature={CONFIG['temperature']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and scheduler\n",
    "total_steps = len(train_loader) * CONFIG['num_epochs']\n",
    "optimizer, scheduler = setup_optimizer_and_scheduler(model, CONFIG, total_steps)\n",
    "\n",
    "print(f\"✓ Optimizer and scheduler created:\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Scheduler: Cosine with warmup\")\n",
    "print(f\"  Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "print(f\"  Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Starting Training\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "try:\n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        # Train\n",
    "        train_loss, train_acc, epoch_history = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler,\n",
    "            epoch, CONFIG, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler,\n",
    "                epoch, val_loss, val_acc, CONFIG,\n",
    "                CONFIG['checkpoint_dir'] / f'best_model{flags_suffix}.pt'\n",
    "            )\n",
    "            print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Save epoch checkpoint\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler,\n",
    "            epoch, val_loss, val_acc, CONFIG,\n",
    "            CONFIG['checkpoint_dir'] / f'checkpoint_epoch_{epoch+1}{flags_suffix}.pt'\n",
    "        )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user\")\n",
    "\n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save history\n",
    "    torch.save(history, CONFIG['log_dir'] / f'training_history{flags_suffix}.pt')\n",
    "    print(f\"\\n✓ Training history saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curves(\n",
    "    history,\n",
    "    save_path=CONFIG['log_dir'] / f'training_curves{flags_suffix}.png',\n",
    "    flags_suffix=flags_suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint_path = CONFIG['checkpoint_dir'] / f'best_model{flags_suffix}.pt'\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = load_checkpoint(checkpoint_path, model, device=device)\n",
    "    print(f\"✓ Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ No checkpoint found, using current model\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Compute Embeddings and Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for validation set\n",
    "image_embeddings, text_embeddings, captions, image_ids = compute_embeddings(\n",
    "    model, val_dataset, batch_size=128, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T)\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "\n",
    "# Image → Text retrieval\n",
    "print(\"\\nImage → Text Retrieval:\")\n",
    "i2t_recalls = compute_recall_at_k(similarity_matrix, k_values=[1, 5, 10])\n",
    "for k, recall in i2t_recalls.items():\n",
    "    print(f\"  Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Text → Image retrieval\n",
    "print(\"\\nText → Image Retrieval:\")\n",
    "t2i_recalls = compute_recall_at_k(similarity_matrix.T, k_values=[1, 5, 10])\n",
    "for k, recall in t2i_recalls.items():\n",
    "    print(f\"  Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Average\n",
    "print(\"\\nAverage (I2T + T2I):\")\n",
    "for k in [1, 5, 10]:\n",
    "    avg_recall = (i2t_recalls[k] + t2i_recalls[k]) / 2\n",
    "    print(f\"  Recall@{k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Text Query → Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text queries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_text_query(query, top_images, top_scores, top_captions):\n",
    "    \"\"\"Visualize retrieval results.\"\"\"\n",
    "    num_images = len(top_images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f'Query: \"{query}\"', fontsize=14, fontweight='bold', y=1.05)\n",
    "    \n",
    "    for i, (img, score, caption) in enumerate(zip(top_images, top_scores, top_captions)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"#{i+1}\\nScore: {score:.3f}\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test queries\n",
    "queries = ['sport', 'a cat', 'food on a plate', 'people playing', 'a car on the street']\n",
    "\n",
    "for query in queries:\n",
    "    top_images, top_scores, top_captions, top_indices = retrieve_images_for_text(\n",
    "        query, model, tokenizer, val_dataset, image_embeddings, top_k=5, device=device\n",
    "    )\n",
    "    visualize_text_query(query, top_images, top_scores, top_captions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Zero-Shot Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify random images\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import denormalize_image\n",
    "\n",
    "def visualize_classification(image, class_labels, probs, predicted_class):\n",
    "    \"\"\"Visualize classification results.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Display image\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Display probabilities\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    colors = ['green' if i == predicted_class else 'skyblue' for i in range(len(class_labels))]\n",
    "    \n",
    "    ax2.barh(y_pos, probs.numpy(), color=colors)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(class_labels)\n",
    "    ax2.set_xlabel('Probability', fontsize=11)\n",
    "    ax2.set_title('Class Probabilities', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    for i, prob in enumerate(probs.numpy()):\n",
    "        ax2.text(prob + 0.02, i, f'{prob*100:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Predicted: '{class_labels[predicted_class]}' ({probs[predicted_class]*100:.2f}%)\")\n",
    "\n",
    "# Test classification\n",
    "class_labels = ['a person', 'an animal', 'a landscape', 'food', 'a vehicle']\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(val_dataset) - 1)\n",
    "    sample = val_dataset[idx]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"True caption: '{sample['caption']}'\")\n",
    "    \n",
    "    probs, pred, scores = zero_shot_classify(\n",
    "        sample['image_raw'], class_labels, model, tokenizer,\n",
    "        use_templates=True, device=device\n",
    "    )\n",
    "    \n",
    "    visualize_classification(sample['image_raw'], class_labels, probs, pred)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation summary\n",
    "print(f\"{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(f\"  Train set: {len(train_dataset):,} images\")\n",
    "print(f\"  Val set: {len(val_dataset):,} images\")\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  BatchNorm: {CONFIG['use_batch_norm']}\")\n",
    "print(f\"  Attention Pooling: {CONFIG['use_attention_pooling']}\")\n",
    "if flags_suffix:\n",
    "    print(f\"  Flags suffix: {flags_suffix}\")\n",
    "\n",
    "print(\"\\nRetrieval Performance:\")\n",
    "print(\"\\n  Image → Text:\")\n",
    "for k, recall in i2t_recalls.items():\n",
    "    print(f\"    Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n  Text → Image:\")\n",
    "for k, recall in t2i_recalls.items():\n",
    "    print(f\"    Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n  Average:\")\n",
    "for k in [1, 5, 10]:\n",
    "    avg_recall = (i2t_recalls[k] + t2i_recalls[k]) / 2\n",
    "    print(f\"    Recall@{k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "eval_results = {\n",
    "    'i2t_recalls': i2t_recalls,\n",
    "    't2i_recalls': t2i_recalls,\n",
    "    'config': CONFIG,\n",
    "    'flags_suffix': flags_suffix\n",
    "}\n",
    "torch.save(eval_results, CONFIG['log_dir'] / f'eval_results{flags_suffix}.pt')\n",
    "print(f\"\\n✓ Evaluation results saved to {CONFIG['log_dir'] / f'eval_results{flags_suffix}.pt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
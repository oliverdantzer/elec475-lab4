{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Model Evaluation and Visualization\n",
    "\n",
    "Evaluate the trained CLIP model with:\n",
    "- **Recall@K Metrics**: Image-to-text and text-to-image retrieval\n",
    "- **Text Query Visualization**: Retrieve top-K images for text queries\n",
    "- **Zero-shot Classification**: Classify images using text prompts\n",
    "\n",
    "## Prerequisites\n",
    "1. Run `coco_dataset_prep.ipynb` to prepare the dataset\n",
    "2. Run `train_clip.ipynb` to train the model\n",
    "3. Have a trained model checkpoint at `/content/checkpoints/best_model.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Configuration\nDATASET_DIR = Path('/content/coco2014')\n\n# Auto-detect checkpoint with flags\ncheckpoint_dir = Path('/content/checkpoints')\navailable_checkpoints = []\n\n# Check for different flag combinations\nfor suffix in ['', '_bn', '_attn', '_bn_attn']:\n    checkpoint_path = checkpoint_dir / f'best_model{suffix}.pt'\n    if checkpoint_path.exists():\n        available_checkpoints.append((suffix, checkpoint_path))\n\n# If checkpoints in subdirectories\nfor subdir in Path('/content').glob('checkpoints*'):\n    for ckpt in subdir.glob('best_model*.pt'):\n        if ckpt not in [c[1] for c in available_checkpoints]:\n            suffix = ckpt.stem.replace('best_model', '')\n            available_checkpoints.append((suffix, ckpt))\n\nif available_checkpoints:\n    print(\"Available checkpoints:\")\n    for i, (suffix, path) in enumerate(available_checkpoints):\n        flags_desc = suffix if suffix else \"(baseline)\"\n        print(f\"  [{i}] {path.name} - {flags_desc}\")\n    \n    # Use the first one by default (you can change this)\n    selected_idx = 0\n    flags_suffix, CHECKPOINT_PATH = available_checkpoints[selected_idx]\n    \n    print(f\"\\n\u2713 Using checkpoint: {CHECKPOINT_PATH.name}\")\n    if flags_suffix:\n        print(f\"  Experimental flags: {flags_suffix}\")\nelse:\n    # Fallback to default\n    CHECKPOINT_PATH = Path('/content/checkpoints/best_model.pt')\n    flags_suffix = \"\"\n    print(f\"\u26a0\ufe0f  No checkpoint found at default location\")\n    print(f\"   Will attempt to load: {CHECKPOINT_PATH}\")\n\nMODEL_NAME = 'openai/clip-vit-base-patch32'\n\nCLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\nCLIP_STD = [0.26862954, 0.26130258, 0.27577711]\nIMAGE_SIZE = 224\n\nprint(f\"\\nDataset directory: {DATASET_DIR}\")\nprint(f\"Checkpoint path: {CHECKPOINT_PATH}\")\nprint(f\"Checkpoint exists: {CHECKPOINT_PATH.exists()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define model architecture (same as training)\n# Must support all flag combinations\n\nclass AttentionPooling(nn.Module):\n    \"\"\"Attention-based pooling for better feature aggregation.\"\"\"\n    def __init__(self, input_dim=2048):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, input_dim // 8),\n            nn.Tanh(),\n            nn.Linear(input_dim // 8, 1)\n        )\n    \n    def forward(self, x):\n        # x: [B, C, H, W]\n        B, C, H, W = x.shape\n        x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # [B, H*W, C]\n        \n        # Compute attention weights\n        attn_weights = self.attention(x_flat)  # [B, H*W, 1]\n        attn_weights = F.softmax(attn_weights, dim=1)\n        \n        # Weighted sum\n        pooled = torch.sum(x_flat * attn_weights, dim=1)  # [B, C]\n        return pooled\n\n\nclass ResNet50ImageEncoder(nn.Module):\n    \"\"\"ResNet50 image encoder with optional attention pooling.\"\"\"\n    def __init__(self, pretrained=True, use_attention_pooling=False):\n        super().__init__()\n        resnet = models.resnet50(pretrained=pretrained)\n        \n        if use_attention_pooling:\n            self.features = nn.Sequential(*list(resnet.children())[:-2])\n            self.pooling = AttentionPooling(input_dim=2048)\n            self.use_attention = True\n        else:\n            self.features = nn.Sequential(*list(resnet.children())[:-1])\n            self.pooling = None\n            self.use_attention = False\n        \n        self.output_dim = 2048\n        \n    def forward(self, x):\n        features = self.features(x)\n        \n        if self.use_attention:\n            pooled = self.pooling(features)\n        else:\n            pooled = features.view(features.size(0), -1)\n        \n        return pooled\n\n\nclass ProjectionHead(nn.Module):\n    \"\"\"2-layer MLP projection head with optional BatchNorm.\"\"\"\n    def __init__(self, input_dim=2048, hidden_dim=1024, output_dim=512, use_batch_norm=False):\n        super().__init__()\n        \n        if use_batch_norm:\n            self.projection = nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.GELU(),\n                nn.Linear(hidden_dim, output_dim),\n                nn.BatchNorm1d(output_dim)\n            )\n        else:\n            self.projection = nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.GELU(),\n                nn.Linear(hidden_dim, output_dim)\n            )\n        \n    def forward(self, x):\n        return self.projection(x)\n\n\nclass CLIPModel(nn.Module):\n    \"\"\"Combined CLIP model.\"\"\"\n    def __init__(self, text_encoder, freeze_text_encoder=True,\n                 use_batch_norm=False, use_attention_pooling=False):\n        super().__init__()\n        self.use_batch_norm = use_batch_norm\n        self.use_attention_pooling = use_attention_pooling\n        \n        self.text_encoder = text_encoder\n        if freeze_text_encoder:\n            for param in self.text_encoder.parameters():\n                param.requires_grad = False\n            self.text_encoder.eval()\n        \n        self.image_encoder = ResNet50ImageEncoder(\n            pretrained=True,\n            use_attention_pooling=use_attention_pooling\n        )\n        \n        self.projection_head = ProjectionHead(\n            input_dim=2048,\n            hidden_dim=1024,\n            output_dim=512,\n            use_batch_norm=use_batch_norm\n        )\n        \n    def encode_image(self, images):\n        features = self.image_encoder(images)\n        embeddings = self.projection_head(features)\n        return F.normalize(embeddings, p=2, dim=1)\n    \n    def encode_text(self, input_ids, attention_mask):\n        with torch.no_grad():\n            outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n            embeddings = outputs.pooler_output\n            return F.normalize(embeddings, p=2, dim=1)\n    \n    def forward(self, images, input_ids, attention_mask):\n        image_embeddings = self.encode_image(images)\n        text_embeddings = self.encode_text(input_ids, attention_mask)\n        return image_embeddings, text_embeddings\n\nprint(\"\u2713 Model architecture defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load tokenizer and create model\nprint(\"Loading CLIP text encoder...\")\ntokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\ntext_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\ntext_encoder = text_encoder.to(device)\n\n# Load checkpoint first to get flags\nuse_batch_norm = False\nuse_attention_pooling = False\n\nif CHECKPOINT_PATH.exists():\n    print(f\"\\nLoading checkpoint from {CHECKPOINT_PATH}...\")\n    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n    \n    # Extract flags from checkpoint\n    if 'flags' in checkpoint:\n        use_batch_norm = checkpoint['flags'].get('use_batch_norm', False)\n        use_attention_pooling = checkpoint['flags'].get('use_attention_pooling', False)\n        print(f\"\\n\ud83d\udd2c Checkpoint trained with experimental flags:\")\n        print(f\"  BatchNorm: {use_batch_norm}\")\n        print(f\"  Attention Pooling: {use_attention_pooling}\")\n    elif 'config' in checkpoint and 'use_batch_norm' in checkpoint['config']:\n        use_batch_norm = checkpoint['config']['use_batch_norm']\n        use_attention_pooling = checkpoint['config']['use_attention_pooling']\n        print(f\"\\n\ud83d\udd2c Checkpoint trained with experimental flags:\")\n        print(f\"  BatchNorm: {use_batch_norm}\")\n        print(f\"  Attention Pooling: {use_attention_pooling}\")\n    else:\n        print(f\"\\n  Using baseline configuration (no experimental flags)\")\n\n# Create model with same architecture as checkpoint\nmodel = CLIPModel(\n    text_encoder=text_encoder,\n    freeze_text_encoder=True,\n    use_batch_norm=use_batch_norm,\n    use_attention_pooling=use_attention_pooling\n)\nmodel = model.to(device)\n\n# Load trained weights\nif CHECKPOINT_PATH.exists():\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"\\n\u2713 Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n    print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"  Validation accuracy: {checkpoint['val_acc']:.4f}\")\nelse:\n    print(\"\u26a0\ufe0f  No checkpoint found. Using untrained model.\")\n\nmodel.eval()\nprint(\"\\n\u2713 Model ready for evaluation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture (same as training)\n",
    "\n",
    "class ResNet50ImageEncoder(nn.Module):\n",
    "    \"\"\"ResNet50 image encoder.\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.output_dim = 2048\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        return features.view(features.size(0), -1)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"2-layer MLP projection head.\"\"\"\n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"Combined CLIP model.\"\"\"\n",
    "    def __init__(self, text_encoder, freeze_text_encoder=True):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if freeze_text_encoder:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.text_encoder.eval()\n",
    "        \n",
    "        self.image_encoder = ResNet50ImageEncoder(pretrained=True)\n",
    "        self.projection_head = ProjectionHead(input_dim=2048, hidden_dim=1024, output_dim=512)\n",
    "        \n",
    "    def encode_image(self, images):\n",
    "        features = self.image_encoder(images)\n",
    "        embeddings = self.projection_head(features)\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.pooler_output\n",
    "            return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        image_embeddings = self.encode_image(images)\n",
    "        text_embeddings = self.encode_text(input_ids, attention_mask)\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "print(\"\u2713 Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and create model\n",
    "print(\"Loading CLIP text encoder...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "# Create model\n",
    "model = CLIPModel(text_encoder=text_encoder, freeze_text_encoder=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained weights\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f\"\\nLoading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\u2713 Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No checkpoint found. Using untrained model.\")\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n\u2713 Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOClipDataset(Dataset):\n",
    "    \"\"\"COCO Dataset for evaluation.\"\"\"\n",
    "    def __init__(self, split='val', dataset_dir=DATASET_DIR, return_all_captions=True):\n",
    "        self.split = split\n",
    "        self.image_dir = dataset_dir / f'{split}2014'\n",
    "        self.cache_file = dataset_dir / f'{split}_text_embeddings.pt'\n",
    "        self.return_all_captions = return_all_captions\n",
    "        \n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "        ])\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        cache = torch.load(self.cache_file)\n",
    "        self.cache_data = cache['data']\n",
    "        \n",
    "        print(f\"Loaded {split} dataset: {len(self.cache_data):,} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.cache_data[idx]\n",
    "        image_id = item['image_id']\n",
    "        embeddings = item['embeddings']\n",
    "        captions = item['captions']\n",
    "        \n",
    "        # Load image\n",
    "        image_filename = f'COCO_{self.split}2014_{image_id:012d}.jpg'\n",
    "        image_path = self.image_dir / image_filename\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.transform(image)\n",
    "        except:\n",
    "            image = None\n",
    "            image_tensor = torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        \n",
    "        if self.return_all_captions:\n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'image_raw': image,\n",
    "                'text_embeddings': embeddings,\n",
    "                'captions': captions,\n",
    "                'image_id': image_id,\n",
    "                'image_path': str(image_path)\n",
    "            }\n",
    "        else:\n",
    "            caption_idx = random.randint(0, len(captions) - 1)\n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'image_raw': image,\n",
    "                'text_embedding': embeddings[caption_idx],\n",
    "                'caption': captions[caption_idx],\n",
    "                'image_id': image_id,\n",
    "                'image_path': str(image_path)\n",
    "            }\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = COCOClipDataset(split='val', return_all_captions=False)\n",
    "print(f\"\u2713 Dataset loaded: {len(val_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Embeddings for Entire Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_embeddings(model, dataset, batch_size=128):\n",
    "    \"\"\"\n",
    "    Compute image and text embeddings for entire dataset.\n",
    "    \n",
    "    Returns:\n",
    "        image_embeddings: [N, 512]\n",
    "        text_embeddings: [N, 512]\n",
    "        captions: List[str] of length N\n",
    "        image_ids: List[int] of length N\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_captions = []\n",
    "    all_image_ids = []\n",
    "    \n",
    "    print(\"Computing embeddings...\")\n",
    "    for batch in tqdm(loader):\n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Encode images\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Store\n",
    "        all_image_embeddings.append(image_embeddings.cpu())\n",
    "        all_text_embeddings.append(text_embeddings.cpu())\n",
    "        all_captions.extend(batch['caption'])\n",
    "        all_image_ids.extend(batch['image_id'].tolist())\n",
    "    \n",
    "    # Concatenate\n",
    "    image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    \n",
    "    print(f\"\u2713 Computed embeddings:\")\n",
    "    print(f\"  Images: {image_embeddings.shape}\")\n",
    "    print(f\"  Texts: {text_embeddings.shape}\")\n",
    "    \n",
    "    return image_embeddings, text_embeddings, all_captions, all_image_ids\n",
    "\n",
    "# Compute embeddings\n",
    "image_embeddings, text_embeddings, captions, image_ids = compute_embeddings(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recall@K Metrics\n",
    "\n",
    "Compute retrieval performance metrics:\n",
    "- **Image \u2192 Text**: Given an image, retrieve matching captions\n",
    "- **Text \u2192 Image**: Given a caption, retrieve matching images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(similarity_matrix, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute Recall@K for retrieval.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: [N, M] similarity scores\n",
    "        k_values: List of K values to compute\n",
    "        \n",
    "    Returns:\n",
    "        recall_dict: Dictionary of {K: recall_value}\n",
    "    \"\"\"\n",
    "    N = similarity_matrix.shape[0]\n",
    "    \n",
    "    # Get top-K indices for each query\n",
    "    # similarity_matrix[i, j] = similarity between query i and candidate j\n",
    "    # For each query, we want to find if the correct match is in top-K\n",
    "    \n",
    "    # Assuming diagonal elements are correct matches (image i \u2194 text i)\n",
    "    top_k_indices = torch.topk(similarity_matrix, k=max(k_values), dim=1, largest=True).indices\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        # Check if correct index (i) is in top-k for query i\n",
    "        correct_in_top_k = torch.any(\n",
    "            top_k_indices[:, :k] == torch.arange(N).unsqueeze(1),\n",
    "            dim=1\n",
    "        )\n",
    "        recall = correct_in_top_k.float().mean().item()\n",
    "        recalls[k] = recall\n",
    "    \n",
    "    return recalls\n",
    "\n",
    "\n",
    "# Compute similarity matrix\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T)  # [N, N]\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "\n",
    "# Image \u2192 Text retrieval\n",
    "print(\"\\nImage \u2192 Text Retrieval:\")\n",
    "i2t_recalls = compute_recall_at_k(similarity_matrix, k_values=[1, 5, 10])\n",
    "for k, recall in i2t_recalls.items():\n",
    "    print(f\"  Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Text \u2192 Image retrieval\n",
    "print(\"\\nText \u2192 Image Retrieval:\")\n",
    "t2i_recalls = compute_recall_at_k(similarity_matrix.T, k_values=[1, 5, 10])\n",
    "for k, recall in t2i_recalls.items():\n",
    "    print(f\"  Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Average recalls\n",
    "print(\"\\nAverage (I2T + T2I):\")\n",
    "for k in [1, 5, 10]:\n",
    "    avg_recall = (i2t_recalls[k] + t2i_recalls[k]) / 2\n",
    "    print(f\"  Recall@{k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Query \u2192 Image Retrieval Visualization\n",
    "\n",
    "Given a text query, retrieve and display the top-K most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(tensor):\n",
    "    \"\"\"Denormalize image tensor for display.\"\"\"\n",
    "    mean = torch.tensor(CLIP_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(CLIP_STD).view(3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def retrieve_images_for_text(query_text, model, tokenizer, dataset, image_embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-K images for a text query.\n",
    "    \n",
    "    Args:\n",
    "        query_text: String text query\n",
    "        model: CLIP model\n",
    "        tokenizer: CLIP tokenizer\n",
    "        dataset: Dataset to retrieve images from\n",
    "        image_embeddings: Pre-computed image embeddings [N, 512]\n",
    "        top_k: Number of images to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        top_images: List of PIL images\n",
    "        top_scores: List of similarity scores\n",
    "        top_captions: List of captions\n",
    "        top_indices: List of dataset indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode query text\n",
    "    inputs = tokenizer(\n",
    "        [query_text],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    query_embedding = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n",
    "    query_embedding = query_embedding.cpu()\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.matmul(query_embedding, image_embeddings.T).squeeze(0)\n",
    "    \n",
    "    # Get top-K\n",
    "    top_scores, top_indices = torch.topk(similarities, k=min(top_k, len(similarities)), largest=True)\n",
    "    \n",
    "    # Retrieve images\n",
    "    top_images = []\n",
    "    top_captions = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        sample = dataset[idx.item()]\n",
    "        top_images.append(sample['image_raw'])\n",
    "        top_captions.append(sample['caption'])\n",
    "    \n",
    "    return top_images, top_scores.tolist(), top_captions, top_indices.tolist()\n",
    "\n",
    "\n",
    "def visualize_text_query_results(query_text, top_images, top_scores, top_captions, figsize=(15, 3)):\n",
    "    \"\"\"Visualize retrieval results for a text query.\"\"\"\n",
    "    num_images = len(top_images)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f'Query: \"{query_text}\"', fontsize=14, fontweight='bold', y=1.05)\n",
    "    \n",
    "    for i, (img, score, caption) in enumerate(zip(top_images, top_scores, top_captions)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add score and caption\n",
    "        title = f\"Rank {i+1}\\nScore: {score:.3f}\"\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "        \n",
    "        # Add caption below\n",
    "        wrapped_caption = '\\n'.join([caption[j:j+25] for j in range(0, len(caption), 25)])\n",
    "        axes[i].text(0.5, -0.15, wrapped_caption, \n",
    "                     transform=axes[i].transAxes,\n",
    "                     ha='center', va='top', fontsize=8,\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\u2713 Text query retrieval functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Retrieve images for text queries\n",
    "queries = ['sport', 'a cat', 'food on a plate', 'people playing', 'a car on the street']\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    top_images, top_scores, top_captions, top_indices = retrieve_images_for_text(\n",
    "        query, model, tokenizer, val_dataset, image_embeddings, top_k=5\n",
    "    )\n",
    "    visualize_text_query_results(query, top_images, top_scores, top_captions)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zero-Shot Image Classification\n",
    "\n",
    "Given an image and a list of class labels, classify the image by computing similarity with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_classify(image, class_labels, model, tokenizer, use_templates=True):\n",
    "    \"\"\"\n",
    "    Classify an image using zero-shot CLIP.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or tensor [3, 224, 224]\n",
    "        class_labels: List of class names (e.g., ['a person', 'an animal'])\n",
    "        model: CLIP model\n",
    "        tokenizer: CLIP tokenizer\n",
    "        use_templates: If True, use prompt templates\n",
    "        \n",
    "    Returns:\n",
    "        probs: Probability distribution over classes\n",
    "        predicted_class: Index of predicted class\n",
    "        class_scores: Raw similarity scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare image\n",
    "    if isinstance(image, Image.Image):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "        ])\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Encode image\n",
    "    image_embedding = model.encode_image(image_tensor)\n",
    "    \n",
    "    # Prepare text prompts\n",
    "    if use_templates:\n",
    "        # Use prompt templates (like CLIP paper)\n",
    "        templates = [\n",
    "            'a photo of {}',\n",
    "            'a picture of {}',\n",
    "            'an image of {}',\n",
    "        ]\n",
    "        texts = []\n",
    "        for label in class_labels:\n",
    "            for template in templates:\n",
    "                texts.append(template.format(label))\n",
    "    else:\n",
    "        texts = class_labels\n",
    "    \n",
    "    # Encode texts\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    text_embeddings = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "    # Compute similarities\n",
    "    if use_templates:\n",
    "        # Average over templates\n",
    "        text_embeddings = text_embeddings.view(len(class_labels), len(templates), -1)\n",
    "        text_embeddings = text_embeddings.mean(dim=1)  # [num_classes, 512]\n",
    "    \n",
    "    similarities = torch.matmul(image_embedding, text_embeddings.T).squeeze(0)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(similarities * 100, dim=0)  # Temperature scaling\n",
    "    \n",
    "    predicted_class = torch.argmax(similarities).item()\n",
    "    \n",
    "    return probs.cpu(), predicted_class, similarities.cpu()\n",
    "\n",
    "\n",
    "def visualize_classification(image, class_labels, probs, predicted_class):\n",
    "    \"\"\"Visualize zero-shot classification results.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Display image\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image_display = denormalize_image(image).permute(1, 2, 0).numpy()\n",
    "        ax1.imshow(image_display)\n",
    "    else:\n",
    "        ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Display probabilities\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    colors = ['green' if i == predicted_class else 'skyblue' for i in range(len(class_labels))]\n",
    "    \n",
    "    ax2.barh(y_pos, probs.numpy(), color=colors)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(class_labels)\n",
    "    ax2.set_xlabel('Probability', fontsize=11)\n",
    "    ax2.set_title('Class Probabilities', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, prob in enumerate(probs.numpy()):\n",
    "        ax2.text(prob + 0.02, i, f'{prob*100:.1f}%', va='center')\n",
    "    \n",
    "    # Highlight prediction\n",
    "    ax2.axhline(predicted_class, color='green', linestyle='--', alpha=0.3, linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Predicted class: '{class_labels[predicted_class]}' ({probs[predicted_class]*100:.2f}%)\")\n",
    "\n",
    "print(\"\u2713 Zero-shot classification functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Classify random images\n",
    "num_examples = 5\n",
    "class_labels = ['a person', 'an animal', 'a landscape', 'food', 'a vehicle']\n",
    "\n",
    "print(f\"Classifying {num_examples} random images...\\n\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    # Get random sample\n",
    "    idx = random.randint(0, len(val_dataset) - 1)\n",
    "    sample = val_dataset[idx]\n",
    "    \n",
    "    image = sample['image_raw']\n",
    "    true_caption = sample['caption']\n",
    "    \n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"True caption: '{true_caption}'\")\n",
    "    \n",
    "    # Classify\n",
    "    probs, predicted_class, scores = zero_shot_classify(\n",
    "        image, class_labels, model, tokenizer, use_templates=True\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_classification(image, class_labels, probs, predicted_class)\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Classification Examples\n",
    "\n",
    "Try your own classification tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Indoor vs Outdoor\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "class_labels = ['an indoor scene', 'an outdoor scene']\n",
    "probs, pred, scores = zero_shot_classify(sample['image_raw'], class_labels, model, tokenizer)\n",
    "\n",
    "print(f\"True caption: '{sample['caption']}'\")\n",
    "visualize_classification(sample['image_raw'], class_labels, probs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Activity classification\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "class_labels = ['people eating', 'people playing sports', 'people working', 'people relaxing']\n",
    "probs, pred, scores = zero_shot_classify(sample['image_raw'], class_labels, model, tokenizer)\n",
    "\n",
    "print(f\"True caption: '{sample['caption']}'\")\n",
    "visualize_classification(sample['image_raw'], class_labels, probs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Object detection\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "class_labels = ['a dog', 'a cat', 'a bird', 'a horse', 'a cow', 'a sheep']\n",
    "probs, pred, scores = zero_shot_classify(sample['image_raw'], class_labels, model, tokenizer)\n",
    "\n",
    "print(f\"True caption: '{sample['caption']}'\")\n",
    "visualize_classification(sample['image_raw'], class_labels, probs, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation summary\n",
    "print(f\"{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(f\"  Validation set size: {len(val_dataset):,} images\")\n",
    "print(f\"  Embedding dimension: {image_embeddings.shape[1]}\")\n",
    "\n",
    "print(\"\\nRetrieval Performance:\")\n",
    "print(\"\\n  Image \u2192 Text Retrieval:\")\n",
    "for k, recall in i2t_recalls.items():\n",
    "    print(f\"    Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n  Text \u2192 Image Retrieval:\")\n",
    "for k, recall in t2i_recalls.items():\n",
    "    print(f\"    Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n  Average Recall:\")\n",
    "for k in [1, 5, 10]:\n",
    "    avg_recall = (i2t_recalls[k] + t2i_recalls[k]) / 2\n",
    "    print(f\"    Recall@{k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nCapabilities Demonstrated:\")\n",
    "print(\"  \u2713 Text query \u2192 Image retrieval\")\n",
    "print(\"  \u2713 Zero-shot image classification\")\n",
    "print(\"  \u2713 Multi-class categorization\")\n",
    "print(\"  \u2713 Prompt template ensembling\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    'i2t_recalls': i2t_recalls,\n",
    "    't2i_recalls': t2i_recalls,\n",
    "    'dataset_size': len(val_dataset),\n",
    "    'embedding_dim': image_embeddings.shape[1],\n",
    "}\n",
    "\n",
    "torch.save(eval_results, '/content/logs/evaluation_results.pt')\n",
    "print(\"\\n\u2713 Evaluation results saved to /content/logs/evaluation_results.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Model Evaluation and Visualization\n",
    "\n",
    "Evaluate the trained CLIP model with:\n",
    "- **Recall@K Metrics**: Image-to-text and text-to-image retrieval\n",
    "- **Text Query Visualization**: Retrieve top-K images for text queries\n",
    "- **Zero-shot Classification**: Classify images using text prompts\n",
    "\n",
    "## Prerequisites\n",
    "1. Run `coco_dataset_prep.ipynb` to prepare the dataset\n",
    "2. Run `train_clip.ipynb` to train the model\n",
    "3. Have a trained model checkpoint at `/content/checkpoints/best_model.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_DIR = Path('/content/coco2014')\n",
    "CHECKPOINT_PATH = Path('/content/checkpoints/best_model.pt')\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
    "CLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Checkpoint exists: {CHECKPOINT_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture (same as training)\n",
    "\n",
    "class ResNet50ImageEncoder(nn.Module):\n",
    "    \"\"\"ResNet50 image encoder.\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.output_dim = 2048\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        return features.view(features.size(0), -1)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"2-layer MLP projection head.\"\"\"\n",
    "    def __init__(self, input_dim=2048, hidden_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"Combined CLIP model.\"\"\"\n",
    "    def __init__(self, text_encoder, freeze_text_encoder=True):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if freeze_text_encoder:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.text_encoder.eval()\n",
    "        \n",
    "        self.image_encoder = ResNet50ImageEncoder(pretrained=True)\n",
    "        self.projection_head = ProjectionHead(input_dim=2048, hidden_dim=1024, output_dim=512)\n",
    "        \n",
    "    def encode_image(self, images):\n",
    "        features = self.image_encoder(images)\n",
    "        embeddings = self.projection_head(features)\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = outputs.pooler_output\n",
    "            return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        image_embeddings = self.encode_image(images)\n",
    "        text_embeddings = self.encode_text(input_ids, attention_mask)\n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "print(\"✓ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and create model\n",
    "print(\"Loading CLIP text encoder...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "# Create model\n",
    "model = CLIPModel(text_encoder=text_encoder, freeze_text_encoder=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained weights\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f\"\\nLoading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"  Validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️  No checkpoint found. Using untrained model.\")\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n✓ Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOClipDataset(Dataset):\n",
    "    \"\"\"COCO Dataset for evaluation.\"\"\"\n",
    "    def __init__(self, split='val', dataset_dir=DATASET_DIR, return_all_captions=True):\n",
    "        self.split = split\n",
    "        self.image_dir = dataset_dir / f'{split}2014'\n",
    "        self.cache_file = dataset_dir / f'{split}_text_embeddings.pt'\n",
    "        self.return_all_captions = return_all_captions\n",
    "        \n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "        ])\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        cache = torch.load(self.cache_file)\n",
    "        self.cache_data = cache['data']\n",
    "        \n",
    "        print(f\"Loaded {split} dataset: {len(self.cache_data):,} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.cache_data[idx]\n",
    "        image_id = item['image_id']\n",
    "        embeddings = item['embeddings']\n",
    "        captions = item['captions']\n",
    "        \n",
    "        # Load image\n",
    "        image_filename = f'COCO_{self.split}2014_{image_id:012d}.jpg'\n",
    "        image_path = self.image_dir / image_filename\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.transform(image)\n",
    "        except:\n",
    "            image = None\n",
    "            image_tensor = torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        \n",
    "        if self.return_all_captions:\n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'image_raw': image,\n",
    "                'text_embeddings': embeddings,\n",
    "                'captions': captions,\n",
    "                'image_id': image_id,\n",
    "                'image_path': str(image_path)\n",
    "            }\n",
    "        else:\n",
    "            caption_idx = random.randint(0, len(captions) - 1)\n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'image_raw': image,\n",
    "                'text_embedding': embeddings[caption_idx],\n",
    "                'caption': captions[caption_idx],\n",
    "                'image_id': image_id,\n",
    "                'image_path': str(image_path)\n",
    "            }\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = COCOClipDataset(split='val', return_all_captions=False)\n",
    "print(f\"✓ Dataset loaded: {len(val_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Embeddings for Entire Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_embeddings(model, dataset, batch_size=128):\n",
    "    \"\"\"\n",
    "    Compute image and text embeddings for entire dataset.\n",
    "    \n",
    "    Returns:\n",
    "        image_embeddings: [N, 512]\n",
    "        text_embeddings: [N, 512]\n",
    "        captions: List[str] of length N\n",
    "        image_ids: List[int] of length N\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    all_image_embeddings = []\n",
    "    all_text_embeddings = []\n",
    "    all_captions = []\n",
    "    all_image_ids = []\n",
    "    \n",
    "    print(\"Computing embeddings...\")\n",
    "    for batch in tqdm(loader):\n",
    "        images = batch['image'].to(device)\n",
    "        text_embeddings = batch['text_embedding'].to(device)\n",
    "        \n",
    "        # Encode images\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        \n",
    "        # Store\n",
    "        all_image_embeddings.append(image_embeddings.cpu())\n",
    "        all_text_embeddings.append(text_embeddings.cpu())\n",
    "        all_captions.extend(batch['caption'])\n",
    "        all_image_ids.extend(batch['image_id'].tolist())\n",
    "    \n",
    "    # Concatenate\n",
    "    image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "    text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
    "    \n",
    "    print(f\"✓ Computed embeddings:\")\n",
    "    print(f\"  Images: {image_embeddings.shape}\")\n",
    "    print(f\"  Texts: {text_embeddings.shape}\")\n",
    "    \n",
    "    return image_embeddings, text_embeddings, all_captions, all_image_ids\n",
    "\n",
    "# Compute embeddings\n",
    "image_embeddings, text_embeddings, captions, image_ids = compute_embeddings(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recall@K Metrics\n",
    "\n",
    "Compute retrieval performance metrics:\n",
    "- **Image → Text**: Given an image, retrieve matching captions\n",
    "- **Text → Image**: Given a caption, retrieve matching images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(similarity_matrix, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute Recall@K for retrieval.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: [N, M] similarity scores\n",
    "        k_values: List of K values to compute\n",
    "        \n",
    "    Returns:\n",
    "        recall_dict: Dictionary of {K: recall_value}\n",
    "    \"\"\"\n",
    "    N = similarity_matrix.shape[0]\n",
    "    \n",
    "    # Get top-K indices for each query\n",
    "    # similarity_matrix[i, j] = similarity between query i and candidate j\n",
    "    # For each query, we want to find if the correct match is in top-K\n",
    "    \n",
    "    # Assuming diagonal elements are correct matches (image i ↔ text i)\n",
    "    top_k_indices = torch.topk(similarity_matrix, k=max(k_values), dim=1, largest=True).indices\n",
    "    \n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        # Check if correct index (i) is in top-k for query i\n",
    "        correct_in_top_k = torch.any(\n",
    "            top_k_indices[:, :k] == torch.arange(N).unsqueeze(1),\n",
    "            dim=1\n",
    "        )\n",
    "        recall = correct_in_top_k.float().mean().item()\n",
    "        recalls[k] = recall\n",
    "    \n",
    "    return recalls\n",
    "\n",
    "\n",
    "# Compute similarity matrix\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T)  # [N, N]\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "\n",
    "# Image → Text retrieval\n",
    "print(\"\\nImage → Text Retrieval:\")\n",
    "i2t_recalls = compute_recall_at_k(similarity_matrix, k_values=[1, 5, 10])\n",
    "for k, recall in i2t_recalls.items():\n",
    "    print(f\"  Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Text → Image retrieval\n",
    "print(\"\\nText → Image Retrieval:\")\n",
    "t2i_recalls = compute_recall_at_k(similarity_matrix.T, k_values=[1, 5, 10])\n",
    "for k, recall in t2i_recalls.items():\n",
    "    print(f\"  Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Average recalls\n",
    "print(\"\\nAverage (I2T + T2I):\")\n",
    "for k in [1, 5, 10]:\n",
    "    avg_recall = (i2t_recalls[k] + t2i_recalls[k]) / 2\n",
    "    print(f\"  Recall@{k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Query → Image Retrieval Visualization\n",
    "\n",
    "Given a text query, retrieve and display the top-K most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(tensor):\n",
    "    \"\"\"Denormalize image tensor for display.\"\"\"\n",
    "    mean = torch.tensor(CLIP_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(CLIP_STD).view(3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def retrieve_images_for_text(query_text, model, tokenizer, dataset, image_embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-K images for a text query.\n",
    "    \n",
    "    Args:\n",
    "        query_text: String text query\n",
    "        model: CLIP model\n",
    "        tokenizer: CLIP tokenizer\n",
    "        dataset: Dataset to retrieve images from\n",
    "        image_embeddings: Pre-computed image embeddings [N, 512]\n",
    "        top_k: Number of images to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        top_images: List of PIL images\n",
    "        top_scores: List of similarity scores\n",
    "        top_captions: List of captions\n",
    "        top_indices: List of dataset indices\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode query text\n",
    "    inputs = tokenizer(\n",
    "        [query_text],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    query_embedding = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n",
    "    query_embedding = query_embedding.cpu()\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.matmul(query_embedding, image_embeddings.T).squeeze(0)\n",
    "    \n",
    "    # Get top-K\n",
    "    top_scores, top_indices = torch.topk(similarities, k=min(top_k, len(similarities)), largest=True)\n",
    "    \n",
    "    # Retrieve images\n",
    "    top_images = []\n",
    "    top_captions = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        sample = dataset[idx.item()]\n",
    "        top_images.append(sample['image_raw'])\n",
    "        top_captions.append(sample['caption'])\n",
    "    \n",
    "    return top_images, top_scores.tolist(), top_captions, top_indices.tolist()\n",
    "\n",
    "\n",
    "def visualize_text_query_results(query_text, top_images, top_scores, top_captions, figsize=(15, 3)):\n",
    "    \"\"\"Visualize retrieval results for a text query.\"\"\"\n",
    "    num_images = len(top_images)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f'Query: \"{query_text}\"', fontsize=14, fontweight='bold', y=1.05)\n",
    "    \n",
    "    for i, (img, score, caption) in enumerate(zip(top_images, top_scores, top_captions)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add score and caption\n",
    "        title = f\"Rank {i+1}\\nScore: {score:.3f}\"\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "        \n",
    "        # Add caption below\n",
    "        wrapped_caption = '\\n'.join([caption[j:j+25] for j in range(0, len(caption), 25)])\n",
    "        axes[i].text(0.5, -0.15, wrapped_caption, \n",
    "                     transform=axes[i].transAxes,\n",
    "                     ha='center', va='top', fontsize=8,\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Text query retrieval functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Retrieve images for text queries\n",
    "queries = ['sport', 'a cat', 'food on a plate', 'people playing', 'a car on the street']\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    top_images, top_scores, top_captions, top_indices = retrieve_images_for_text(\n",
    "        query, model, tokenizer, val_dataset, image_embeddings, top_k=5\n",
    "    )\n",
    "    visualize_text_query_results(query, top_images, top_scores, top_captions)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zero-Shot Image Classification\n",
    "\n",
    "Given an image and a list of class labels, classify the image by computing similarity with each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_classify(image, class_labels, model, tokenizer, use_templates=True):\n",
    "    \"\"\"\n",
    "    Classify an image using zero-shot CLIP.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or tensor [3, 224, 224]\n",
    "        class_labels: List of class names (e.g., ['a person', 'an animal'])\n",
    "        model: CLIP model\n",
    "        tokenizer: CLIP tokenizer\n",
    "        use_templates: If True, use prompt templates\n",
    "        \n",
    "    Returns:\n",
    "        probs: Probability distribution over classes\n",
    "        predicted_class: Index of predicted class\n",
    "        class_scores: Raw similarity scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare image\n",
    "    if isinstance(image, Image.Image):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "        ])\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Encode image\n",
    "    image_embedding = model.encode_image(image_tensor)\n",
    "    \n",
    "    # Prepare text prompts\n",
    "    if use_templates:\n",
    "        # Use prompt templates (like CLIP paper)\n",
    "        templates = [\n",
    "            'a photo of {}',\n",
    "            'a picture of {}',\n",
    "            'an image of {}',\n",
    "        ]\n",
    "        texts = []\n",
    "        for label in class_labels:\n",
    "            for template in templates:\n",
    "                texts.append(template.format(label))\n",
    "    else:\n",
    "        texts = class_labels\n",
    "    \n",
    "    # Encode texts\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    text_embeddings = model.encode_text(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "    # Compute similarities\n",
    "    if use_templates:\n",
    "        # Average over templates\n",
    "        text_embeddings = text_embeddings.view(len(class_labels), len(templates), -1)\n",
    "        text_embeddings = text_embeddings.mean(dim=1)  # [num_classes, 512]\n",
    "    \n",
    "    similarities = torch.matmul(image_embedding, text_embeddings.T).squeeze(0)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(similarities * 100, dim=0)  # Temperature scaling\n",
    "    \n",
    "    predicted_class = torch.argmax(similarities).item()\n",
    "    \n",
    "    return probs.cpu(), predicted_class, similarities.cpu()\n",
    "\n",
    "\n",
    "def visualize_classification(image, class_labels, probs, predicted_class):\n",
    "    \"\"\"Visualize zero-shot classification results.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Display image\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image_display = denormalize_image(image).permute(1, 2, 0).numpy()\n",
    "        ax1.imshow(image_display)\n",
    "    else:\n",
    "        ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Display probabilities\n",
    "    y_pos = np.arange(len(class_labels))\n",
    "    colors = ['green' if i == predicted_class else 'skyblue' for i in range(len(class_labels))]\n",
    "    \n",
    "    ax2.barh(y_pos, probs.numpy(), color=colors)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(class_labels)\n",
    "    ax2.set_xlabel('Probability', fontsize=11)\n",
    "    ax2.set_title('Class Probabilities', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, prob in enumerate(probs.numpy()):\n",
    "        ax2.text(prob + 0.02, i, f'{prob*100:.1f}%', va='center')\n",
    "    \n",
    "    # Highlight prediction\n",
    "    ax2.axhline(predicted_class, color='green', linestyle='--', alpha=0.3, linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Predicted class: '{class_labels[predicted_class]}' ({probs[predicted_class]*100:.2f}%)\")\n",
    "\n",
    "print(\"✓ Zero-shot classification functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Classify random images\n",
    "num_examples = 5\n",
    "class_labels = ['a person', 'an animal', 'a landscape', 'food', 'a vehicle']\n",
    "\n",
    "print(f\"Classifying {num_examples} random images...\\n\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    # Get random sample\n",
    "    idx = random.randint(0, len(val_dataset) - 1)\n",
    "    sample = val_dataset[idx]\n",
    "    \n",
    "    image = sample['image_raw']\n",
    "    true_caption = sample['caption']\n",
    "    \n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"True caption: '{true_caption}'\")\n",
    "    \n",
    "    # Classify\n",
    "    probs, predicted_class, scores = zero_shot_classify(\n",
    "        image, class_labels, model, tokenizer, use_templates=True\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_classification(image, class_labels, probs, predicted_class)\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Classification Examples\n",
    "\n",
    "Try your own classification tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Indoor vs Outdoor\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "class_labels = ['an indoor scene', 'an outdoor scene']\n",
    "probs, pred, scores = zero_shot_classify(sample['image_raw'], class_labels, model, tokenizer)\n",
    "\n",
    "print(f\"True caption: '{sample['caption']}'\")\n",
    "visualize_classification(sample['image_raw'], class_labels, probs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Activity classification\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "class_labels = ['people eating', 'people playing sports', 'people working', 'people relaxing']\n",
    "probs, pred, scores = zero_shot_classify(sample['image_raw'], class_labels, model, tokenizer)\n",
    "\n",
    "print(f\"True caption: '{sample['caption']}'\")\n",
    "visualize_classification(sample['image_raw'], class_labels, probs, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Object detection\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "class_labels = ['a dog', 'a cat', 'a bird', 'a horse', 'a cow', 'a sheep']\n",
    "probs, pred, scores = zero_shot_classify(sample['image_raw'], class_labels, model, tokenizer)\n",
    "\n",
    "print(f\"True caption: '{sample['caption']}'\")\n",
    "visualize_classification(sample['image_raw'], class_labels, probs, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation summary\n",
    "print(f\"{'='*60}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(f\"  Validation set size: {len(val_dataset):,} images\")\n",
    "print(f\"  Embedding dimension: {image_embeddings.shape[1]}\")\n",
    "\n",
    "print(\"\\nRetrieval Performance:\")\n",
    "print(\"\\n  Image → Text Retrieval:\")\n",
    "for k, recall in i2t_recalls.items():\n",
    "    print(f\"    Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n  Text → Image Retrieval:\")\n",
    "for k, recall in t2i_recalls.items():\n",
    "    print(f\"    Recall@{k}: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n  Average Recall:\")\n",
    "for k in [1, 5, 10]:\n",
    "    avg_recall = (i2t_recalls[k] + t2i_recalls[k]) / 2\n",
    "    print(f\"    Recall@{k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nCapabilities Demonstrated:\")\n",
    "print(\"  ✓ Text query → Image retrieval\")\n",
    "print(\"  ✓ Zero-shot image classification\")\n",
    "print(\"  ✓ Multi-class categorization\")\n",
    "print(\"  ✓ Prompt template ensembling\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    'i2t_recalls': i2t_recalls,\n",
    "    't2i_recalls': t2i_recalls,\n",
    "    'dataset_size': len(val_dataset),\n",
    "    'embedding_dim': image_embeddings.shape[1],\n",
    "}\n",
    "\n",
    "torch.save(eval_results, '/content/logs/evaluation_results.pt')\n",
    "print(\"\\n✓ Evaluation results saved to /content/logs/evaluation_results.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

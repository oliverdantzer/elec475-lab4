{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS COCO 2014 Dataset Preparation for CLIP Fine-tuning\n",
    "\n",
    "This notebook prepares the COCO 2014 dataset for fine-tuning CLIP models.\n",
    "\n",
    "## Features:\n",
    "- Automatic Kaggle dataset download\n",
    "- CLIP-specific image preprocessing (224x224, normalized)\n",
    "- Text embedding caching for efficient training\n",
    "- PyTorch Dataset implementation\n",
    "- Verification and visualization\n",
    "\n",
    "## Dataset Info:\n",
    "- Training images: ~82,783\n",
    "- Validation images: ~40,504\n",
    "- Multiple captions per image\n",
    "- Source: COCO 2014 from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.30.0 torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q pillow kaggle pycocotools matplotlib tqdm\n",
    "\n",
    "print(\"✓ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML and Image processing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Transformers for CLIP\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kaggle Authentication Setup\n",
    "\n",
    "**Instructions:**\n",
    "1. Download your `kaggle.json` from [Kaggle Account Settings](https://www.kaggle.com/settings)\n",
    "2. Upload it using the file upload button in Colab (left sidebar → Files → Upload)\n",
    "3. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle authentication\n",
    "kaggle_json_path = 'kaggle.json'\n",
    "\n",
    "# Check if kaggle.json exists in current directory\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    print(\"⚠️  kaggle.json not found!\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Download kaggle.json from https://www.kaggle.com/settings\")\n",
    "    print(\"2. Upload it to this Colab environment using the file browser\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "else:\n",
    "    # Create .kaggle directory in home\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy kaggle.json to ~/.kaggle/\n",
    "    target_path = kaggle_dir / 'kaggle.json'\n",
    "    !cp {kaggle_json_path} {target_path}\n",
    "    \n",
    "    # Set proper permissions\n",
    "    !chmod 600 {target_path}\n",
    "    \n",
    "    print(\"✓ Kaggle authentication configured successfully!\")\n",
    "    print(f\"  Credentials saved to: {target_path}\")\n",
    "    \n",
    "    # Verify authentication\n",
    "    !kaggle --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download COCO 2014 Dataset\n",
    "\n",
    "This will download ~13GB of data. Download time depends on your connection speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_DIR = Path('/content/coco2014')\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "print(\"Downloading COCO 2014 dataset from Kaggle...\")\n",
    "print(\"This may take 10-20 minutes depending on connection speed.\\n\")\n",
    "\n",
    "!kaggle datasets download -d jeffaudi/coco-2014-dataset-for-yolov3 -p {DATASET_DIR} --unzip\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset download complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify directory structure\n",
    "expected_dirs = ['train2014', 'val2014', 'annotations']\n",
    "print(\"\\nVerifying directory structure:\")\n",
    "\n",
    "for dir_name in expected_dirs:\n",
    "    dir_path = DATASET_DIR / dir_name\n",
    "    if dir_path.exists():\n",
    "        if dir_name == 'annotations':\n",
    "            files = list(dir_path.glob('*.json'))\n",
    "            print(f\"  ✓ {dir_name}/: {len(files)} JSON files\")\n",
    "        else:\n",
    "            files = list(dir_path.glob('*.jpg'))\n",
    "            print(f\"  ✓ {dir_name}/: {len(files):,} images\")\n",
    "    else:\n",
    "        print(f\"  ✗ {dir_name}/ NOT FOUND\")\n",
    "\n",
    "# Check for caption files\n",
    "annotations_dir = DATASET_DIR / 'annotations'\n",
    "caption_files = {\n",
    "    'train': annotations_dir / 'captions_train2014.json',\n",
    "    'val': annotations_dir / 'captions_val2014.json'\n",
    "}\n",
    "\n",
    "print(\"\\nCaption files:\")\n",
    "for split, path in caption_files.items():\n",
    "    if path.exists():\n",
    "        # Load and print statistics\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"  ✓ {path.name}\")\n",
    "        print(f\"    - Images: {len(data['images']):,}\")\n",
    "        print(f\"    - Captions: {len(data['annotations']):,}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {path.name} NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load CLIP Model Components\n",
    "\n",
    "Loading pre-trained CLIP model from HuggingFace: `openai/clip-vit-base-patch32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP components\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "print(f\"Loading CLIP model: {MODEL_NAME}...\")\n",
    "\n",
    "# Load tokenizer and text model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to GPU and set to eval mode\n",
    "text_encoder = text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "\n",
    "print(f\"✓ CLIP text encoder loaded successfully!\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in text_encoder.parameters()):,}\")\n",
    "print(f\"  Text embedding dimension: {text_encoder.config.hidden_size}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Image Transforms\n",
    "\n",
    "CLIP requires specific normalization values:\n",
    "- Input size: 224×224\n",
    "- Mean: [0.48145466, 0.4578275, 0.40821073]\n",
    "- Std: [0.26862954, 0.26130258, 0.27577711]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP normalization constants\n",
    "CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
    "CLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Define image transformation pipeline\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "])\n",
    "\n",
    "# Define inverse transform for visualization\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    mean = torch.tensor(CLIP_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(CLIP_STD).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "print(\"✓ Image transforms configured:\")\n",
    "print(f\"  Input size: {IMAGE_SIZE}×{IMAGE_SIZE}\")\n",
    "print(f\"  Normalization mean: {CLIP_MEAN}\")\n",
    "print(f\"  Normalization std: {CLIP_STD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode and Cache Training Captions\n",
    "\n",
    "This cell encodes all training captions using CLIP's text encoder and caches them to disk.\n",
    "This saves GPU memory and computation time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_cache_captions(split='train', batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode captions using CLIP text encoder and cache to disk.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val'\n",
    "        batch_size: Number of captions to process at once\n",
    "    \"\"\"\n",
    "    # Load captions JSON\n",
    "    caption_file = DATASET_DIR / 'annotations' / f'captions_{split}2014.json'\n",
    "    print(f\"Loading captions from {caption_file.name}...\")\n",
    "    \n",
    "    with open(caption_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Organize captions by image_id\n",
    "    print(\"Organizing captions by image_id...\")\n",
    "    image_to_captions = defaultdict(list)\n",
    "    \n",
    "    for annotation in coco_data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        caption = annotation['caption']\n",
    "        image_to_captions[image_id].append(caption)\n",
    "    \n",
    "    print(f\"Found {len(image_to_captions)} unique images with captions\")\n",
    "    \n",
    "    # Prepare data for encoding\n",
    "    cache_data = []\n",
    "    \n",
    "    # Process each image's captions\n",
    "    print(f\"\\nEncoding captions (batch_size={batch_size})...\")\n",
    "    \n",
    "    image_ids = list(image_to_captions.keys())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=f\"Encoding {split} captions\"):\n",
    "            captions = image_to_captions[img_id]\n",
    "            \n",
    "            # Tokenize all captions for this image\n",
    "            inputs = tokenizer(\n",
    "                captions,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device and encode\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = text_encoder(**inputs)\n",
    "            \n",
    "            # Get pooled output (CLS token representation)\n",
    "            embeddings = outputs.pooler_output.cpu()\n",
    "            \n",
    "            # Store data\n",
    "            cache_data.append({\n",
    "                'image_id': img_id,\n",
    "                'embeddings': embeddings,  # Shape: [num_captions, 512]\n",
    "                'captions': captions\n",
    "            })\n",
    "            \n",
    "            # Periodically clear CUDA cache\n",
    "            if len(cache_data) % 1000 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save cache to disk\n",
    "    cache_file = DATASET_DIR / f'{split}_text_embeddings.pt'\n",
    "    print(f\"\\nSaving cache to {cache_file}...\")\n",
    "    \n",
    "    torch.save({\n",
    "        'data': cache_data,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'embedding_dim': text_encoder.config.hidden_size\n",
    "    }, cache_file)\n",
    "    \n",
    "    # Print statistics\n",
    "    cache_size_mb = cache_file.stat().st_size / (1024 * 1024)\n",
    "    total_captions = sum(len(item['captions']) for item in cache_data)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cache created successfully!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  File: {cache_file.name}\")\n",
    "    print(f\"  Size: {cache_size_mb:.2f} MB\")\n",
    "    print(f\"  Images: {len(cache_data):,}\")\n",
    "    print(f\"  Total captions: {total_captions:,}\")\n",
    "    print(f\"  Avg captions/image: {total_captions/len(cache_data):.2f}\")\n",
    "    print(f\"  Embedding dimension: {text_encoder.config.hidden_size}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return cache_file\n",
    "\n",
    "# Encode training captions\n",
    "train_cache_file = encode_and_cache_captions(split='train', batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encode and Cache Validation Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode validation captions\n",
    "val_cache_file = encode_and_cache_captions(split='val', batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define PyTorch Dataset Class\n",
    "\n",
    "Custom Dataset class that:\n",
    "- Loads images on-the-fly (memory efficient)\n",
    "- Retrieves pre-computed text embeddings from cache\n",
    "- Handles multiple captions per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOClipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO Dataset for CLIP fine-tuning.\n",
    "    \n",
    "    Returns:\n",
    "        image: Preprocessed image tensor [3, 224, 224]\n",
    "        text_embedding: Pre-computed CLIP text embedding [512]\n",
    "        caption: Original caption text (for reference)\n",
    "        image_id: COCO image ID\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', transform=None, return_all_captions=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: 'train' or 'val'\n",
    "            transform: Image transforms to apply\n",
    "            return_all_captions: If True, return all captions. If False, randomly select one.\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform or image_transforms\n",
    "        self.return_all_captions = return_all_captions\n",
    "        \n",
    "        # Set paths\n",
    "        self.image_dir = DATASET_DIR / f'{split}2014'\n",
    "        self.cache_file = DATASET_DIR / f'{split}_text_embeddings.pt'\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        print(f\"Loading cached embeddings from {self.cache_file.name}...\")\n",
    "        cache = torch.load(self.cache_file)\n",
    "        self.cache_data = cache['data']\n",
    "        self.embedding_dim = cache['embedding_dim']\n",
    "        \n",
    "        # Build index: image_id -> cache index\n",
    "        self.image_id_to_idx = {\n",
    "            item['image_id']: idx \n",
    "            for idx, item in enumerate(self.cache_data)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ Loaded {len(self.cache_data)} images\")\n",
    "        print(f\"  ✓ Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get cached data\n",
    "        item = self.cache_data[idx]\n",
    "        image_id = item['image_id']\n",
    "        embeddings = item['embeddings']  # [num_captions, 512]\n",
    "        captions = item['captions']\n",
    "        \n",
    "        # Load image\n",
    "        image_filename = f'COCO_{self.split}2014_{image_id:012d}.jpg'\n",
    "        image_path = self.image_dir / image_filename\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Select caption(s)\n",
    "        if self.return_all_captions:\n",
    "            # Return all captions and embeddings\n",
    "            return {\n",
    "                'image': image,\n",
    "                'text_embeddings': embeddings,\n",
    "                'captions': captions,\n",
    "                'image_id': image_id\n",
    "            }\n",
    "        else:\n",
    "            # Randomly select one caption\n",
    "            caption_idx = random.randint(0, len(captions) - 1)\n",
    "            return {\n",
    "                'image': image,\n",
    "                'text_embedding': embeddings[caption_idx],\n",
    "                'caption': captions[caption_idx],\n",
    "                'image_id': image_id\n",
    "            }\n",
    "\n",
    "print(\"✓ COCOClipDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Dataset Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and val datasets\n",
    "print(\"Creating dataset instances...\\n\")\n",
    "\n",
    "train_dataset = COCOClipDataset(split='train', transform=image_transforms)\n",
    "print()\n",
    "val_dataset = COCOClipDataset(split='val', transform=image_transforms)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Dataset Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set size: {len(train_dataset):,} images\")\n",
    "print(f\"Validation set size: {len(val_dataset):,} images\")\n",
    "print(f\"Total: {len(train_dataset) + len(val_dataset):,} images\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Test loading a single sample\n",
    "print(\"\\nTesting dataset loading...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  ✓ Image shape: {sample['image'].shape}\")\n",
    "print(f\"  ✓ Text embedding shape: {sample['text_embedding'].shape}\")\n",
    "print(f\"  ✓ Caption: \\\"{sample['caption']}\\\"\")\n",
    "print(f\"  ✓ Image ID: {sample['image_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Random Image-Caption Pairs\n",
    "\n",
    "Verify that images and captions are correctly loaded and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=6, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize random samples from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: COCOClipDataset instance\n",
    "        num_samples: Number of samples to display\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Select random indices\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    rows = (num_samples + 2) // 3\n",
    "    cols = min(3, num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        # Get sample\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image']\n",
    "        caption = sample['caption']\n",
    "        image_id = sample['image_id']\n",
    "        \n",
    "        # Denormalize image for display\n",
    "        image_display = denormalize(image)\n",
    "        image_display = torch.clamp(image_display, 0, 1)\n",
    "        image_display = image_display.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(image_display)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add caption as title\n",
    "        wrapped_caption = '\\n'.join(\n",
    "            [caption[i:i+40] for i in range(0, len(caption), 40)]\n",
    "        )\n",
    "        ax.set_title(f\"ID: {image_id}\\n{wrapped_caption}\", \n",
    "                     fontsize=9, pad=10)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for ax in axes[num_samples:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nDisplayed {num_samples} random samples\")\n",
    "    print(f\"  Image tensor shape: {sample['image'].shape}\")\n",
    "    print(f\"  Text embedding shape: {sample['text_embedding'].shape}\")\n",
    "\n",
    "# Visualize training samples\n",
    "print(\"Training Set Samples:\")\n",
    "print(\"=\"*60)\n",
    "visualize_samples(train_dataset, num_samples=6)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Validation Set Samples:\")\n",
    "print(\"=\"*60)\n",
    "visualize_samples(val_dataset, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Dataset Statistics and Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 13. Define ResNet50 Image Encoder\n\nImplement a ResNet50 backbone with pretrained ImageNet weights as the image encoder."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torchvision.models as models\n\nclass ResNet50ImageEncoder(nn.Module):\n    \"\"\"\n    ResNet50-based image encoder with pretrained ImageNet weights.\n    \n    Extracts features from the last convolutional layer before the\n    classification head (2048-dim features for ResNet50).\n    \"\"\"\n    \n    def __init__(self, pretrained=True):\n        \"\"\"\n        Args:\n            pretrained: If True, load ImageNet pretrained weights\n        \"\"\"\n        super(ResNet50ImageEncoder, self).__init__()\n        \n        # Load ResNet50 with pretrained weights\n        resnet = models.resnet50(pretrained=pretrained)\n        \n        # Remove the final classification layer (fc)\n        # Keep all layers up to avgpool\n        self.features = nn.Sequential(*list(resnet.children())[:-1])\n        \n        # Output dimension from ResNet50 after avgpool\n        self.output_dim = 2048\n        \n        print(f\"✓ ResNet50 image encoder initialized\")\n        print(f\"  Pretrained: {pretrained}\")\n        print(f\"  Output dimension: {self.output_dim}\")\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Image tensor [batch_size, 3, 224, 224]\n            \n        Returns:\n            features: Image features [batch_size, 2048]\n        \"\"\"\n        # Extract features\n        features = self.features(x)  # [batch_size, 2048, 1, 1]\n        \n        # Flatten\n        features = features.view(features.size(0), -1)  # [batch_size, 2048]\n        \n        return features\n\n# Create ResNet50 encoder\nimage_encoder = ResNet50ImageEncoder(pretrained=True)\nimage_encoder = image_encoder.to(device)\nimage_encoder.eval()\n\n# Count parameters\ntotal_params = sum(p.numel() for p in image_encoder.parameters())\ntrainable_params = sum(p.numel() for p in image_encoder.parameters() if p.requires_grad)\n\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 14. Define Projection Head\n\nTwo-layer MLP with GELU activation that projects ResNet50 features (2048-dim) to CLIP embedding space (512-dim).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ProjectionHead(nn.Module):\n    \"\"\"\n    Two-layer MLP projection head with GELU activation.\n    \n    Projects image features from ResNet50 (2048-dim) to CLIP embedding space (512-dim).\n    \"\"\"\n    \n    def __init__(self, input_dim=2048, hidden_dim=1024, output_dim=512):\n        \"\"\"\n        Args:\n            input_dim: Input feature dimension (ResNet50 output)\n            hidden_dim: Hidden layer dimension\n            output_dim: Output embedding dimension (CLIP space)\n        \"\"\"\n        super(ProjectionHead, self).__init__()\n        \n        self.projection = nn.Sequential(\n            # First linear layer\n            nn.Linear(input_dim, hidden_dim),\n            nn.GELU(),\n            # Second linear layer\n            nn.Linear(hidden_dim, output_dim)\n        )\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        \n        print(f\"✓ Projection head initialized\")\n        print(f\"  Architecture: {input_dim} → {hidden_dim} → {output_dim}\")\n        print(f\"  Activation: GELU\")\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Image features [batch_size, 2048]\n            \n        Returns:\n            embeddings: Projected embeddings [batch_size, 512]\n        \"\"\"\n        return self.projection(x)\n\n# Create projection head\nprojection_head = ProjectionHead(\n    input_dim=2048,  # ResNet50 output\n    hidden_dim=1024,  # Hidden dimension\n    output_dim=512  # CLIP embedding dimension\n)\nprojection_head = projection_head.to(device)\n\n# Count parameters\nproj_params = sum(p.numel() for p in projection_head.parameters())\nprint(f\"  Total parameters: {proj_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 15. Define Combined CLIP Model\n\nCombine the image encoder, projection head, and frozen text encoder into a single model.\n\n**Training Strategy:**\n- Text encoder: **FROZEN** (pretrained CLIP weights)\n- Image encoder: **TRAINABLE** (fine-tune from ImageNet weights)\n- Projection head: **TRAINABLE** (learn alignment to CLIP space)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class CLIPModel(nn.Module):\n    \"\"\"\n    Combined CLIP model for fine-tuning.\n    \n    Architecture:\n        - Text Encoder: Frozen pretrained CLIP text model\n        - Image Encoder: Trainable ResNet50 (ImageNet init)\n        - Projection Head: Trainable 2-layer MLP\n    \"\"\"\n    \n    def __init__(self, text_encoder, image_encoder, projection_head, freeze_text_encoder=True):\n        \"\"\"\n        Args:\n            text_encoder: Pretrained CLIP text encoder\n            image_encoder: ResNet50 image encoder\n            projection_head: Projection head to map to CLIP space\n            freeze_text_encoder: If True, freeze text encoder parameters\n        \"\"\"\n        super(CLIPModel, self).__init__()\n        \n        self.text_encoder = text_encoder\n        self.image_encoder = image_encoder\n        self.projection_head = projection_head\n        \n        # Freeze text encoder\n        if freeze_text_encoder:\n            for param in self.text_encoder.parameters():\n                param.requires_grad = False\n            self.text_encoder.eval()\n            print(\"✓ Text encoder frozen (eval mode)\")\n        \n        # Print parameter statistics\n        self._print_parameter_stats()\n        \n    def _print_parameter_stats(self):\n        \"\"\"Print parameter statistics for each component\"\"\"\n        text_params = sum(p.numel() for p in self.text_encoder.parameters())\n        text_trainable = sum(p.numel() for p in self.text_encoder.parameters() if p.requires_grad)\n        \n        image_params = sum(p.numel() for p in self.image_encoder.parameters())\n        image_trainable = sum(p.numel() for p in self.image_encoder.parameters() if p.requires_grad)\n        \n        proj_params = sum(p.numel() for p in self.projection_head.parameters())\n        proj_trainable = sum(p.numel() for p in self.projection_head.parameters() if p.requires_grad)\n        \n        total_params = text_params + image_params + proj_params\n        total_trainable = text_trainable + image_trainable + proj_trainable\n        \n        print(f\"\\n{'='*60}\")\n        print(\"Model Parameter Statistics\")\n        print(f\"{'='*60}\")\n        print(f\"Text Encoder:\")\n        print(f\"  Total: {text_params:,} | Trainable: {text_trainable:,}\")\n        print(f\"Image Encoder (ResNet50):\")\n        print(f\"  Total: {image_params:,} | Trainable: {image_trainable:,}\")\n        print(f\"Projection Head:\")\n        print(f\"  Total: {proj_params:,} | Trainable: {proj_trainable:,}\")\n        print(f\"{'-'*60}\")\n        print(f\"Total Parameters: {total_params:,}\")\n        print(f\"Trainable Parameters: {total_trainable:,} ({100*total_trainable/total_params:.1f}%)\")\n        print(f\"{'='*60}\\n\")\n        \n    def encode_image(self, images):\n        \"\"\"\n        Encode images to CLIP embedding space.\n        \n        Args:\n            images: Image tensor [batch_size, 3, 224, 224]\n            \n        Returns:\n            image_embeddings: Normalized embeddings [batch_size, 512]\n        \"\"\"\n        # Extract features with ResNet50\n        features = self.image_encoder(images)  # [batch_size, 2048]\n        \n        # Project to CLIP space\n        embeddings = self.projection_head(features)  # [batch_size, 512]\n        \n        # L2 normalize (important for contrastive learning)\n        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n        \n        return embeddings\n    \n    def encode_text(self, input_ids, attention_mask):\n        \"\"\"\n        Encode text to CLIP embedding space.\n        \n        Args:\n            input_ids: Token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            \n        Returns:\n            text_embeddings: Normalized embeddings [batch_size, 512]\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            embeddings = outputs.pooler_output  # [batch_size, 512]\n            \n            # L2 normalize\n            embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n            \n        return embeddings\n    \n    def forward(self, images, input_ids, attention_mask):\n        \"\"\"\n        Forward pass for training.\n        \n        Args:\n            images: Image tensor [batch_size, 3, 224, 224]\n            input_ids: Token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            \n        Returns:\n            image_embeddings: [batch_size, 512]\n            text_embeddings: [batch_size, 512]\n        \"\"\"\n        image_embeddings = self.encode_image(images)\n        text_embeddings = self.encode_text(input_ids, attention_mask)\n        \n        return image_embeddings, text_embeddings\n\n\n# Create the combined model\nprint(\"Creating combined CLIP model...\\n\")\n\nmodel = CLIPModel(\n    text_encoder=text_encoder,\n    image_encoder=image_encoder,\n    projection_head=projection_head,\n    freeze_text_encoder=True\n)\n\nmodel = model.to(device)\n\nprint(\"✓ CLIP model created successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 16. Model Verification\n\nTest the model with a sample batch to ensure all components work correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test the model with a sample batch\nprint(\"Testing model with sample batch...\\n\")\n\n# Create a small dataloader for testing\ntest_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nbatch = next(iter(test_loader))\n\n# Get batch data\nimages = batch['image'].to(device)\ncaptions = batch['caption']\n\n# Tokenize captions\ninputs = tokenizer(\n    captions,\n    padding=True,\n    truncation=True,\n    max_length=77,\n    return_tensors='pt'\n)\ninput_ids = inputs['input_ids'].to(device)\nattention_mask = inputs['attention_mask'].to(device)\n\n# Forward pass\nmodel.eval()\nwith torch.no_grad():\n    image_embeddings, text_embeddings = model(images, input_ids, attention_mask)\n\nprint(f\"{'='*60}\")\nprint(\"Model Verification Results\")\nprint(f\"{'='*60}\")\nprint(f\"Batch size: {images.shape[0]}\")\nprint(f\"\\nInput shapes:\")\nprint(f\"  Images: {images.shape}\")\nprint(f\"  Token IDs: {input_ids.shape}\")\nprint(f\"\\nOutput shapes:\")\nprint(f\"  Image embeddings: {image_embeddings.shape}\")\nprint(f\"  Text embeddings: {text_embeddings.shape}\")\nprint(f\"\\nEmbedding statistics:\")\nprint(f\"  Image embeddings norm: {torch.norm(image_embeddings, dim=1).mean():.4f}\")\nprint(f\"  Text embeddings norm: {torch.norm(text_embeddings, dim=1).mean():.4f}\")\nprint(f\"  (Should be ~1.0 due to L2 normalization)\")\n\n# Compute cosine similarities\nsimilarities = torch.matmul(image_embeddings, text_embeddings.T)\nprint(f\"\\nCosine similarities (image x text):\")\nprint(f\"  Shape: {similarities.shape}\")\nprint(f\"  Diagonal (matched pairs): {torch.diag(similarities).tolist()}\")\nprint(f\"  Mean similarity: {similarities.mean():.4f}\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"✓ Model verification complete!\")\nprint(f\"{'='*60}\\n\")\n\n# Display one example\nprint(f\"Example from batch:\")\nprint(f\"  Caption: \\\"{captions[0]}\\\"\")\nprint(f\"  Image-Text similarity: {similarities[0, 0]:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary and Next Steps\n\nCongratulations! You've successfully prepared the COCO 2014 dataset and built the CLIP model architecture.\n\n### What's Ready:\n✅ **Dataset**: 82K+ training images, 40K+ validation images  \n✅ **Text Embeddings**: All captions pre-encoded and cached  \n✅ **Image Encoder**: ResNet50 with ImageNet weights  \n✅ **Projection Head**: 2-layer MLP (2048 → 1024 → 512)  \n✅ **Text Encoder**: Frozen CLIP model  \n✅ **Model**: Complete architecture verified and functional\n\n### What's Next:\n\nTo complete the CLIP fine-tuning lab, you need to implement:\n\n1. **InfoNCE Contrastive Loss**\n   ```python\n   # Pseudocode\n   logits = image_embeddings @ text_embeddings.T  # [B, B]\n   labels = torch.arange(B)  # Diagonal elements are matches\n   loss = cross_entropy(logits / temperature, labels)\n   ```\n\n2. **Training Loop**\n   - Optimizer for image encoder + projection head (e.g., AdamW)\n   - Learning rate scheduler\n   - Training epochs with progress tracking\n   - Validation evaluation\n\n3. **Evaluation Metrics**\n   - Recall@K (K=1, 5, 10)\n   - Image-to-text retrieval\n   - Text-to-image retrieval\n\n4. **Qualitative Analysis**\n   - Visualize retrieval results\n   - Test on novel images\n   - Analyze failure cases\n\n### Key Variables Available:\n- `model` - Complete CLIP model\n- `train_dataset`, `val_dataset` - PyTorch datasets\n- `tokenizer` - CLIP tokenizer\n- `device` - GPU/CPU device",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS COCO 2014 Dataset Preparation for CLIP Fine-tuning\n",
    "\n",
    "This notebook prepares the COCO 2014 dataset for fine-tuning CLIP models.\n",
    "\n",
    "## Features:\n",
    "- Automatic Kaggle dataset download\n",
    "- CLIP-specific image preprocessing (224x224, normalized)\n",
    "- Text embedding caching for efficient training\n",
    "- PyTorch Dataset implementation\n",
    "- Verification and visualization\n",
    "\n",
    "## Dataset Info:\n",
    "- Training images: ~82,783\n",
    "- Validation images: ~40,504\n",
    "- Multiple captions per image\n",
    "- Source: COCO 2014 from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.30.0 torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q pillow kaggle pycocotools matplotlib tqdm\n",
    "\n",
    "print(\"\u2713 All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML and Image processing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Transformers for CLIP\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kaggle Authentication Setup\n",
    "\n",
    "**Instructions:**\n",
    "1. Download your `kaggle.json` from [Kaggle Account Settings](https://www.kaggle.com/settings)\n",
    "2. Upload it using the file upload button in Colab (left sidebar \u2192 Files \u2192 Upload)\n",
    "3. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle authentication\n",
    "kaggle_json_path = 'kaggle.json'\n",
    "\n",
    "# Check if kaggle.json exists in current directory\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    print(\"\u26a0\ufe0f  kaggle.json not found!\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Download kaggle.json from https://www.kaggle.com/settings\")\n",
    "    print(\"2. Upload it to this Colab environment using the file browser\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "else:\n",
    "    # Create .kaggle directory in home\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy kaggle.json to ~/.kaggle/\n",
    "    target_path = kaggle_dir / 'kaggle.json'\n",
    "    !cp {kaggle_json_path} {target_path}\n",
    "    \n",
    "    # Set proper permissions\n",
    "    !chmod 600 {target_path}\n",
    "    \n",
    "    print(\"\u2713 Kaggle authentication configured successfully!\")\n",
    "    print(f\"  Credentials saved to: {target_path}\")\n",
    "    \n",
    "    # Verify authentication\n",
    "    !kaggle --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download COCO 2014 Dataset\n",
    "\n",
    "This will download ~13GB of data. Download time depends on your connection speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_DIR = Path('/content/coco2014')\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "print(\"Downloading COCO 2014 dataset from Kaggle...\")\n",
    "print(\"This may take 10-20 minutes depending on connection speed.\\n\")\n",
    "\n",
    "!kaggle datasets download -d jeffaudi/coco-2014-dataset-for-yolov3 -p {DATASET_DIR} --unzip\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset download complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify directory structure\n",
    "expected_dirs = ['train2014', 'val2014', 'annotations']\n",
    "print(\"\\nVerifying directory structure:\")\n",
    "\n",
    "for dir_name in expected_dirs:\n",
    "    dir_path = DATASET_DIR / dir_name\n",
    "    if dir_path.exists():\n",
    "        if dir_name == 'annotations':\n",
    "            files = list(dir_path.glob('*.json'))\n",
    "            print(f\"  \u2713 {dir_name}/: {len(files)} JSON files\")\n",
    "        else:\n",
    "            files = list(dir_path.glob('*.jpg'))\n",
    "            print(f\"  \u2713 {dir_name}/: {len(files):,} images\")\n",
    "    else:\n",
    "        print(f\"  \u2717 {dir_name}/ NOT FOUND\")\n",
    "\n",
    "# Check for caption files\n",
    "annotations_dir = DATASET_DIR / 'annotations'\n",
    "caption_files = {\n",
    "    'train': annotations_dir / 'captions_train2014.json',\n",
    "    'val': annotations_dir / 'captions_val2014.json'\n",
    "}\n",
    "\n",
    "print(\"\\nCaption files:\")\n",
    "for split, path in caption_files.items():\n",
    "    if path.exists():\n",
    "        # Load and print statistics\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"  \u2713 {path.name}\")\n",
    "        print(f\"    - Images: {len(data['images']):,}\")\n",
    "        print(f\"    - Captions: {len(data['annotations']):,}\")\n",
    "    else:\n",
    "        print(f\"  \u2717 {path.name} NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load CLIP Model Components\n",
    "\n",
    "Loading pre-trained CLIP model from HuggingFace: `openai/clip-vit-base-patch32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP components\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "print(f\"Loading CLIP model: {MODEL_NAME}...\")\n",
    "\n",
    "# Load tokenizer and text model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to GPU and set to eval mode\n",
    "text_encoder = text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "\n",
    "print(f\"\u2713 CLIP text encoder loaded successfully!\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in text_encoder.parameters()):,}\")\n",
    "print(f\"  Text embedding dimension: {text_encoder.config.hidden_size}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Image Transforms\n",
    "\n",
    "CLIP requires specific normalization values:\n",
    "- Input size: 224\u00d7224\n",
    "- Mean: [0.48145466, 0.4578275, 0.40821073]\n",
    "- Std: [0.26862954, 0.26130258, 0.27577711]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP normalization constants\n",
    "CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
    "CLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Define image transformation pipeline\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD)\n",
    "])\n",
    "\n",
    "# Define inverse transform for visualization\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    mean = torch.tensor(CLIP_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(CLIP_STD).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "print(\"\u2713 Image transforms configured:\")\n",
    "print(f\"  Input size: {IMAGE_SIZE}\u00d7{IMAGE_SIZE}\")\n",
    "print(f\"  Normalization mean: {CLIP_MEAN}\")\n",
    "print(f\"  Normalization std: {CLIP_STD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode and Cache Training Captions\n",
    "\n",
    "This cell encodes all training captions using CLIP's text encoder and caches them to disk.\n",
    "This saves GPU memory and computation time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_cache_captions(split='train', batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode captions using CLIP text encoder and cache to disk.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val'\n",
    "        batch_size: Number of captions to process at once\n",
    "    \"\"\"\n",
    "    # Load captions JSON\n",
    "    caption_file = DATASET_DIR / 'annotations' / f'captions_{split}2014.json'\n",
    "    print(f\"Loading captions from {caption_file.name}...\")\n",
    "    \n",
    "    with open(caption_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Organize captions by image_id\n",
    "    print(\"Organizing captions by image_id...\")\n",
    "    image_to_captions = defaultdict(list)\n",
    "    \n",
    "    for annotation in coco_data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        caption = annotation['caption']\n",
    "        image_to_captions[image_id].append(caption)\n",
    "    \n",
    "    print(f\"Found {len(image_to_captions)} unique images with captions\")\n",
    "    \n",
    "    # Prepare data for encoding\n",
    "    cache_data = []\n",
    "    \n",
    "    # Process each image's captions\n",
    "    print(f\"\\nEncoding captions (batch_size={batch_size})...\")\n",
    "    \n",
    "    image_ids = list(image_to_captions.keys())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids, desc=f\"Encoding {split} captions\"):\n",
    "            captions = image_to_captions[img_id]\n",
    "            \n",
    "            # Tokenize all captions for this image\n",
    "            inputs = tokenizer(\n",
    "                captions,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device and encode\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = text_encoder(**inputs)\n",
    "            \n",
    "            # Get pooled output (CLS token representation)\n",
    "            embeddings = outputs.pooler_output.cpu()\n",
    "            \n",
    "            # Store data\n",
    "            cache_data.append({\n",
    "                'image_id': img_id,\n",
    "                'embeddings': embeddings,  # Shape: [num_captions, 512]\n",
    "                'captions': captions\n",
    "            })\n",
    "            \n",
    "            # Periodically clear CUDA cache\n",
    "            if len(cache_data) % 1000 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save cache to disk\n",
    "    cache_file = DATASET_DIR / f'{split}_text_embeddings.pt'\n",
    "    print(f\"\\nSaving cache to {cache_file}...\")\n",
    "    \n",
    "    torch.save({\n",
    "        'data': cache_data,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'embedding_dim': text_encoder.config.hidden_size\n",
    "    }, cache_file)\n",
    "    \n",
    "    # Print statistics\n",
    "    cache_size_mb = cache_file.stat().st_size / (1024 * 1024)\n",
    "    total_captions = sum(len(item['captions']) for item in cache_data)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cache created successfully!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  File: {cache_file.name}\")\n",
    "    print(f\"  Size: {cache_size_mb:.2f} MB\")\n",
    "    print(f\"  Images: {len(cache_data):,}\")\n",
    "    print(f\"  Total captions: {total_captions:,}\")\n",
    "    print(f\"  Avg captions/image: {total_captions/len(cache_data):.2f}\")\n",
    "    print(f\"  Embedding dimension: {text_encoder.config.hidden_size}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return cache_file\n",
    "\n",
    "# Encode training captions\n",
    "train_cache_file = encode_and_cache_captions(split='train', batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encode and Cache Validation Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode validation captions\n",
    "val_cache_file = encode_and_cache_captions(split='val', batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define PyTorch Dataset Class\n",
    "\n",
    "Custom Dataset class that:\n",
    "- Loads images on-the-fly (memory efficient)\n",
    "- Retrieves pre-computed text embeddings from cache\n",
    "- Handles multiple captions per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOClipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO Dataset for CLIP fine-tuning.\n",
    "    \n",
    "    Returns:\n",
    "        image: Preprocessed image tensor [3, 224, 224]\n",
    "        text_embedding: Pre-computed CLIP text embedding [512]\n",
    "        caption: Original caption text (for reference)\n",
    "        image_id: COCO image ID\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', transform=None, return_all_captions=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: 'train' or 'val'\n",
    "            transform: Image transforms to apply\n",
    "            return_all_captions: If True, return all captions. If False, randomly select one.\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform or image_transforms\n",
    "        self.return_all_captions = return_all_captions\n",
    "        \n",
    "        # Set paths\n",
    "        self.image_dir = DATASET_DIR / f'{split}2014'\n",
    "        self.cache_file = DATASET_DIR / f'{split}_text_embeddings.pt'\n",
    "        \n",
    "        # Load cached embeddings\n",
    "        print(f\"Loading cached embeddings from {self.cache_file.name}...\")\n",
    "        cache = torch.load(self.cache_file)\n",
    "        self.cache_data = cache['data']\n",
    "        self.embedding_dim = cache['embedding_dim']\n",
    "        \n",
    "        # Build index: image_id -> cache index\n",
    "        self.image_id_to_idx = {\n",
    "            item['image_id']: idx \n",
    "            for idx, item in enumerate(self.cache_data)\n",
    "        }\n",
    "        \n",
    "        print(f\"  \u2713 Loaded {len(self.cache_data)} images\")\n",
    "        print(f\"  \u2713 Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cache_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get cached data\n",
    "        item = self.cache_data[idx]\n",
    "        image_id = item['image_id']\n",
    "        embeddings = item['embeddings']  # [num_captions, 512]\n",
    "        captions = item['captions']\n",
    "        \n",
    "        # Load image\n",
    "        image_filename = f'COCO_{self.split}2014_{image_id:012d}.jpg'\n",
    "        image_path = self.image_dir / image_filename\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Select caption(s)\n",
    "        if self.return_all_captions:\n",
    "            # Return all captions and embeddings\n",
    "            return {\n",
    "                'image': image,\n",
    "                'text_embeddings': embeddings,\n",
    "                'captions': captions,\n",
    "                'image_id': image_id\n",
    "            }\n",
    "        else:\n",
    "            # Randomly select one caption\n",
    "            caption_idx = random.randint(0, len(captions) - 1)\n",
    "            return {\n",
    "                'image': image,\n",
    "                'text_embedding': embeddings[caption_idx],\n",
    "                'caption': captions[caption_idx],\n",
    "                'image_id': image_id\n",
    "            }\n",
    "\n",
    "print(\"\u2713 COCOClipDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Dataset Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and val datasets\n",
    "print(\"Creating dataset instances...\\n\")\n",
    "\n",
    "train_dataset = COCOClipDataset(split='train', transform=image_transforms)\n",
    "print()\n",
    "val_dataset = COCOClipDataset(split='val', transform=image_transforms)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Dataset Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set size: {len(train_dataset):,} images\")\n",
    "print(f\"Validation set size: {len(val_dataset):,} images\")\n",
    "print(f\"Total: {len(train_dataset) + len(val_dataset):,} images\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Test loading a single sample\n",
    "print(\"\\nTesting dataset loading...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  \u2713 Image shape: {sample['image'].shape}\")\n",
    "print(f\"  \u2713 Text embedding shape: {sample['text_embedding'].shape}\")\n",
    "print(f\"  \u2713 Caption: \\\"{sample['caption']}\\\"\")\n",
    "print(f\"  \u2713 Image ID: {sample['image_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Random Image-Caption Pairs\n",
    "\n",
    "Verify that images and captions are correctly loaded and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=6, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize random samples from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: COCOClipDataset instance\n",
    "        num_samples: Number of samples to display\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Select random indices\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    rows = (num_samples + 2) // 3\n",
    "    cols = min(3, num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        # Get sample\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image']\n",
    "        caption = sample['caption']\n",
    "        image_id = sample['image_id']\n",
    "        \n",
    "        # Denormalize image for display\n",
    "        image_display = denormalize(image)\n",
    "        image_display = torch.clamp(image_display, 0, 1)\n",
    "        image_display = image_display.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(image_display)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add caption as title\n",
    "        wrapped_caption = '\\n'.join(\n",
    "            [caption[i:i+40] for i in range(0, len(caption), 40)]\n",
    "        )\n",
    "        ax.set_title(f\"ID: {image_id}\\n{wrapped_caption}\", \n",
    "                     fontsize=9, pad=10)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for ax in axes[num_samples:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nDisplayed {num_samples} random samples\")\n",
    "    print(f\"  Image tensor shape: {sample['image'].shape}\")\n",
    "    print(f\"  Text embedding shape: {sample['text_embedding'].shape}\")\n",
    "\n",
    "# Visualize training samples\n",
    "print(\"Training Set Samples:\")\n",
    "print(\"=\"*60)\n",
    "visualize_samples(train_dataset, num_samples=6)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Validation Set Samples:\")\n",
    "print(\"=\"*60)\n",
    "visualize_samples(val_dataset, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Dataset Statistics and Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def dataset_statistics(dataset, name='Dataset', num_check=100):\n    \"\"\"\n    Compute and display dataset statistics.\n    \n    Args:\n        dataset: COCOClipDataset instance\n        name: Dataset name for display\n        num_check: Number of samples to check for integrity\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"{name} Statistics\")\n    print(f\"{'='*60}\")\n    \n    # Basic stats\n    print(f\"Total samples: {len(dataset):,}\")\n    \n    # Check a subset for integrity\n    print(f\"\\nChecking {num_check} random samples for integrity...\")\n    \n    indices = random.sample(range(len(dataset)), min(num_check, len(dataset)))\n    \n    valid_count = 0\n    image_shapes = []\n    embedding_shapes = []\n    caption_lengths = []\n    \n    for idx in tqdm(indices, desc=\"Validating\"):\n        try:\n            sample = dataset[idx]\n            \n            # Check shapes\n            img_shape = sample['image'].shape\n            emb_shape = sample['text_embedding'].shape\n            cap_len = len(sample['caption'])\n            \n            image_shapes.append(img_shape)\n            embedding_shapes.append(emb_shape)\n            caption_lengths.append(cap_len)\n            \n            # Check for expected shapes\n            assert img_shape == (3, 224, 224), f\"Invalid image shape: {img_shape}\"\n            assert emb_shape == (512,), f\"Invalid embedding shape: {emb_shape}\"\n            assert cap_len > 0, \"Empty caption\"\n            \n            valid_count += 1\n            \n        except Exception as e:\n            print(f\"  \u2717 Error at index {idx}: {e}\")\n    \n    print(f\"\\nIntegrity Check Results:\")\n    print(f\"  Valid samples: {valid_count}/{num_check}\")\n    print(f\"  Success rate: {100*valid_count/num_check:.2f}%\")\n    \n    if valid_count > 0:\n        print(f\"\\nShape Statistics:\")\n        print(f\"  Image shape: {image_shapes[0]} (all samples)\")\n        print(f\"  Text embedding shape: {embedding_shapes[0]} (all samples)\")\n        print(f\"  Caption length range: {min(caption_lengths)}-{max(caption_lengths)} chars\")\n        print(f\"  Average caption length: {sum(caption_lengths)/len(caption_lengths):.1f} chars\")\n    \n    print(f\"{'='*60}\")\n\n# Run statistics on both datasets\ndataset_statistics(train_dataset, name='Training Set', num_check=100)\ndataset_statistics(val_dataset, name='Validation Set', num_check=100)\n\n# Final summary\nprint(f\"\\n{'='*60}\")\nprint(\"Dataset Preparation Complete!\")\nprint(f\"{'='*60}\")\nprint(\"\u2713 Images downloaded and organized\")\nprint(\"\u2713 Text embeddings cached\")\nprint(\"\u2713 PyTorch datasets created\")\nprint(\"\u2713 Data integrity verified\")\nprint(\"\\nNext step: Run train_clip.ipynb to train the model!\")\nprint(f\"{'='*60}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}